{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gymnasium\n# !pip install \"gymnasium[atari, accept-rom-license]\"\n# !apt-get install -y swig\n# !pip install gymnasium[box2d]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.650427Z","iopub.execute_input":"2026-01-10T15:16:55.650863Z","iopub.status.idle":"2026-01-10T15:16:55.655450Z","shell.execute_reply.started":"2026-01-10T15:16:55.650828Z","shell.execute_reply":"2026-01-10T15:16:55.654409Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# ===========================\n# IMPORTS & GLOBAL SETUP\n# ===========================\nimport os\nimport json\nimport csv\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nfrom collections import deque\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport torch.nn.functional as F\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.657204Z","iopub.execute_input":"2026-01-10T15:16:55.657750Z","iopub.status.idle":"2026-01-10T15:16:55.702511Z","shell.execute_reply.started":"2026-01-10T15:16:55.657724Z","shell.execute_reply":"2026-01-10T15:16:55.701538Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"import warnings\nimport logging\n\n# -----------------------\n# WARNINGS UNTERDR√úCKEN\n# -----------------------\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Gym / MoviePy Logging leiser machen\nlogging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\nlogging.getLogger(\"moviepy\").setLevel(logging.ERROR)\n\n# SDL / Pygame Headless Fix (verhindert XDG_RUNTIME_DIR Meldung)\nos.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.703661Z","iopub.execute_input":"2026-01-10T15:16:55.703911Z","iopub.status.idle":"2026-01-10T15:16:55.725594Z","shell.execute_reply.started":"2026-01-10T15:16:55.703891Z","shell.execute_reply":"2026-01-10T15:16:55.724586Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"env_name = \"LunarLander-v3\"\nenv = gym.make(env_name)\nstate_shape = env.observation_space.shape\nstate_size = env.observation_space.shape[0]\nnumber_actions = env.action_space.n\nprint('State shape: ', state_shape)\nprint('State size: ', state_size)\nprint('Number of actions: ', number_actions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.727740Z","iopub.execute_input":"2026-01-10T15:16:55.727995Z","iopub.status.idle":"2026-01-10T15:16:55.746145Z","shell.execute_reply.started":"2026-01-10T15:16:55.727974Z","shell.execute_reply":"2026-01-10T15:16:55.745038Z"}},"outputs":[{"name":"stdout","text":"State shape:  (8,)\nState size:  8\nNumber of actions:  4\n","output_type":"stream"}],"execution_count":107},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    \"\"\"\n    Dynamisches MLP f√ºr DQN.\n    Linear + ReLU nur zwischen Hidden-Layern\n    \"\"\"\n    def __init__(self, state_dim, action_dim, layer_sizes=[64, 8, 64, 64]):\n        super().__init__()\n        torch.manual_seed(42)\n\n        layers = []\n        input_size = state_dim\n        for i, size in enumerate(layer_sizes):\n            layers.append(nn.Linear(input_size, size))\n            # ReLU nur nach Hidden-Layern, nicht nach dem letzten Layer\n            if i != len(layer_sizes) - 1:\n                layers.append(nn.ReLU())\n            input_size = size\n\n        # Output Layer\n        layers.append(nn.Linear(input_size, action_dim))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.747758Z","iopub.execute_input":"2026-01-10T15:16:55.748113Z","iopub.status.idle":"2026-01-10T15:16:55.765786Z","shell.execute_reply.started":"2026-01-10T15:16:55.748063Z","shell.execute_reply":"2026-01-10T15:16:55.764818Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"class ReplayBuffer:\n    \"\"\"\n    Speichert vergangene Erfahrungen:\n    (state, action, reward, next_state, done)\n\n    Warum?\n    - bricht zeitliche Korrelationen\n    - alte Erfahrungen k√∂nnen mehrfach gelernt werden\n    \"\"\"\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, experience):\n        \"\"\"\n        F√ºgt eine Erfahrung hinzu.\n        Wenn der Speicher voll ist ‚Üí die √§lteste fliegt raus.\n        \"\"\"\n        self.memory.append(experience)\n        if len(self.memory) > self.capacity:\n            self.memory.pop(0)\n\n    def sample(self, batch_size):\n        \"\"\"\n        Zieht zuf√§llige Erfahrungen f√ºr ein Lern-Update\n        \"\"\"\n        batch = random.sample(self.memory, batch_size)\n\n        states      = torch.tensor(np.vstack([e[0] for e in batch]), dtype=torch.float32).to(self.device)\n        actions     = torch.tensor(np.vstack([e[1] for e in batch]), dtype=torch.long).to(self.device)\n        rewards     = torch.tensor(np.vstack([e[2] for e in batch]), dtype=torch.float32).to(self.device)\n        next_states = torch.tensor(np.vstack([e[3] for e in batch]), dtype=torch.float32).to(self.device)\n        dones       = torch.tensor(np.vstack([e[4] for e in batch]), dtype=torch.float32).to(self.device)\n\n        return states, actions, rewards, next_states, dones\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.767253Z","iopub.execute_input":"2026-01-10T15:16:55.767609Z","iopub.status.idle":"2026-01-10T15:16:55.786455Z","shell.execute_reply.started":"2026-01-10T15:16:55.767584Z","shell.execute_reply":"2026-01-10T15:16:55.785394Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"class DQNAgent:\n    \"\"\"\n    DQN-Agent mit dynamischem QNetwork (beliebige Layer-Architektur)\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        learning_rate,\n        buffer_size,\n        batch_size,\n        gamma,\n        tau,\n        layer_info=None,  # <-- Neue Option f√ºr dynamische Architekturen\n    ):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.batch_size = batch_size\n        self.gamma = gamma          # Discount-Faktor\n        self.tau = tau              # Soft-Update-Faktor bzw interpolation faktor\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.seed = SEED\n\n        # Standard-Layer falls nichts angegeben\n        if layer_info is None:\n            layer_info = [64, 64, 64]\n\n        # Online-Netzwerk\n        self.q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n\n        # Target-Netzwerk\n        self.target_q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n        self.target_q_network.load_state_dict(self.q_network.state_dict())  # initial synchronisieren\n\n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n        # Replay Buffer\n        self.replay_buffer = ReplayBuffer(buffer_size)\n\n        self.step_counter = 0\n\n    def select_action(self, state, epsilon = 0.):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        self.q_network.eval()\n        with torch.no_grad():\n            q_values = self.q_network(state)\n        self.q_network.train()\n\n        if random.random() > epsilon:\n            return q_values.argmax(dim=1).item()\n        else:\n            return random.randrange(self.action_dim)\n\n    def step(self, state, action, reward, next_state, done):\n        self.replay_buffer.add((state, action, reward, next_state, done))\n        self.step_counter += 1\n\n        # Nicht bei jedem Schritt lernen (stabiler)\n        if self.step_counter % 4 == 0:\n            if len(self.replay_buffer.memory) >= self.batch_size:\n                batch = self.replay_buffer.sample(self.batch_size)\n                self.learn(batch)\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states, dones = experiences\n\n        with torch.no_grad():\n            next_q_values = self.target_q_network(next_states).max(dim=1, keepdim=True)[0]\n            q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n\n        q_expected = self.q_network(states).gather(1, actions)\n\n        loss = F.mse_loss(q_expected, q_targets)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.soft_update()\n\n    def soft_update(self):\n        for target_param, local_param in zip(\n            self.target_q_network.parameters(),\n            self.q_network.parameters()\n        ):\n            target_param.data.copy_(\n                self.tau * local_param.data +\n                (1.0 - self.tau) * target_param.data\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.789184Z","iopub.execute_input":"2026-01-10T15:16:55.789730Z","iopub.status.idle":"2026-01-10T15:16:55.815075Z","shell.execute_reply.started":"2026-01-10T15:16:55.789691Z","shell.execute_reply":"2026-01-10T15:16:55.813881Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"def get_layer_info(agent_or_model):\n    \"\"\"\n    Gibt nur die Gr√∂√üe der Linear-Layer zur√ºck, z.B. [64, 64]\n    \"\"\"\n    if hasattr(agent_or_model, \"q_network\"):\n        model = agent_or_model.q_network\n    else:\n        model = agent_or_model\n\n    layer_sizes = []\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear) and layer.out_features != model.model[-1].out_features:\n            # Letzter Linear-Layer (Output) ignorieren\n            layer_sizes.append(layer.out_features)\n    return layer_sizes\n\n# -------------------------------\n# load_model_bundle: rekonstruiert Linear+ReLU Modell\n# -------------------------------\ndef save_model_bundle(model, save_dir, state_dim, action_dim, fitness=None, generation=None):\n    os.makedirs(save_dir, exist_ok=True)\n    # 1Ô∏è‚É£ Weights speichern\n    torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n\n    # 2Ô∏è‚É£ Meta speichern\n    meta = {\n        \"state_dim\": int(state_dim),\n        \"action_dim\": int(action_dim),\n        \"layer_info\": get_layer_info(model),\n        \"fitness\": float(fitness) if fitness is not None else None,\n        \"generation\": int(generation) if generation is not None else None,\n        \"seed\": int(SEED) if SEED is not None else None,\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    with open(os.path.join(save_dir, \"meta.json\"), \"w\") as f:\n        json.dump(meta, f, indent=4)\n    print(f\"‚úì Model saved to {save_dir}\")\n\ndef load_model_bundle(model_dir, device=\"cpu\"):\n    # Meta laden\n    with open(os.path.join(model_dir, \"meta.json\"), \"r\") as f:\n        meta = json.load(f)\n    state_dim = meta[\"state_dim\"]\n    action_dim = meta[\"action_dim\"]\n    layer_info = meta[\"layer_info\"]\n\n    # Modell rekonstruieren\n    model = QNetwork(state_dim, action_dim, layer_info).to(device)\n\n    # Gewichte laden (reihenfolgebasiert)\n    state_dict_saved = torch.load(os.path.join(model_dir, \"model.pth\"), map_location=device)\n    state_dict_model = model.state_dict()\n    for k_model, k_saved in zip(state_dict_model.keys(), state_dict_saved.keys()):\n        state_dict_model[k_model] = state_dict_saved[k_saved]\n    model.load_state_dict(state_dict_model)\n\n    return model, meta\n\n\ndef init_training_run(env_name, base_dir=\"training_history\"):\n    \"\"\"\n    Erstellt einen neuen Run-Ordner mit Timestamp\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    run_dir = os.path.join(base_dir, f\"{env_name}_{timestamp}\")\n    os.makedirs(run_dir, exist_ok=True)\n\n    print(f\"üìÅ Neuer Trainingslauf: {run_dir}\")\n    return run_dir\n\ndef init_training_log(run_dir):\n    \"\"\"\n    Erstellt CSV-Logdatei f√ºr einen Trainingslauf\n    \"\"\"\n    csv_path = os.path.join(run_dir, \"training_log.csv\")\n\n    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n        writer.writerow([\n            \"Environment\",\n            \"Generation\",\n            \"Model_Architecture\",\n            \"Fitness\",\n            \"Trained_Episodes\",\n        ])\n\n    print(f\"üìä Training-Log erstellt: {csv_path}\")\n    return csv_path\n    \ndef append_generation_log(\n    csv_path,\n    env_name,\n    generation,\n    hidden_sizes,\n    fitness,\n    episodes_per_individual,\n):\n    with open(csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n\n        writer.writerow([\n            env_name,\n            generation,\n            str(hidden_sizes),\n            f\"{fitness:.2f}\",\n            generation * episodes_per_individual,\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.816830Z","iopub.execute_input":"2026-01-10T15:16:55.817219Z","iopub.status.idle":"2026-01-10T15:16:55.841829Z","shell.execute_reply.started":"2026-01-10T15:16:55.817183Z","shell.execute_reply":"2026-01-10T15:16:55.840841Z"}},"outputs":[],"execution_count":111},{"cell_type":"code","source":"class PopulationMember:\n    def __init__(self, agent):\n        self.agent = agent\n        self.total_reward = 0.0\n        self.episode_rewards = []\n\ndef select_parents(sorted_population):\n    return sorted_population[0], sorted_population[1]\n\ndef check_architecture_dominance(\n    architecture_win_counter,\n    threshold=6,\n):\n    for arch, wins in architecture_win_counter.items():\n        if wins >= threshold:\n            return list(arch)\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.843113Z","iopub.execute_input":"2026-01-10T15:16:55.843468Z","iopub.status.idle":"2026-01-10T15:16:55.865432Z","shell.execute_reply.started":"2026-01-10T15:16:55.843435Z","shell.execute_reply":"2026-01-10T15:16:55.864495Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"# =========================\n# üöÄ MUTATION\n# =========================\ndef mutate_architecture(\n    layer_info, \n    n_mutations=1, \n    min_neurons=8, \n    max_neurons=128\n):\n    \"\"\"\n    Mutiert ein Layer-Array n_mutations Mal.\n    Optionen pro Mutation:\n    1Ô∏è‚É£ Dupliziere oder l√∂sche eine zuf√§llige Schicht (Layer-Anzahl begrenzt auf min_layers/max_layers)\n    2Ô∏è‚É£ Neuronen *2 oder /2 (begrenzt auf min_neurons/max_neurons)\n    \"\"\"\n    new_layers = layer_info.copy()\n    for _ in range(n_mutations):\n        if len(new_layers) == 0:\n            # Falls alle Layer gel√∂scht wurden, mindestens 1 Layer wieder hinzuf√ºgen\n            new_layers.append(min_neurons)\n            continue\n\n        idx = random.randint(0, len(new_layers)-1)\n        op = random.choice([\"layer_op\", \"scale\"])\n\n        if op == \"layer_op\":\n            # Duplizieren oder l√∂schen, Layer-Anzahl beachten\n            if len(new_layers) == 1:\n                # Nur duplizieren m√∂glich\n                if len(new_layers) < max_layers:\n                    new_layers.insert(idx, new_layers[idx])\n            else:\n                if random.random() < 0.5:\n                    # duplizieren (nur wenn max_layers nicht √ºberschritten)\n                    if len(new_layers) < max_layers:\n                        new_layers.insert(idx, new_layers[idx])\n                else:\n                    # l√∂schen (nur wenn min_layers nicht unterschritten)\n                    if len(new_layers) > min_layers:\n                        new_layers.pop(idx)\n        else:\n            # Neuronen *2 oder /2\n            factor = random.choice([0.5, 2])\n            new_layers[idx] = max(min_neurons, min(max_neurons, int(new_layers[idx] * factor)))\n\n    return new_layers\n\n\ndef create_initial_population(population_size, state_dim, action_dim, min_layers=1, max_layers=10,\n                              min_neurons=8, max_neurons=128):\n    \"\"\"\n    Initialisiert Population. Layer-Gr√∂√üe als Faktor von min_neurons.\n    \"\"\"\n    population = []\n    for _ in range(population_size):\n        num_layers = random.randint(min_layers, max_layers)\n        layers = [random.randint(1, max_neurons // min_neurons) * min_neurons for _ in range(num_layers)]\n        agent = DQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            learning_rate=learning_rate,\n            buffer_size=buffer_size,\n            batch_size=batch_size,\n            gamma=gamma,\n            tau=tau,\n            layer_info=layers\n        )\n        population.append(PopulationMember(agent))\n    return population\n\ndef partial_load_state_dict_flexible(target, src_sd):\n    tgt_sd = target.state_dict()\n    for k, v in src_sd.items():\n        if k in tgt_sd:\n            if tgt_sd[k].shape == v.shape:\n                # exakt gleiche Shape ‚Üí kopieren\n                tgt_sd[k] = v\n            else:\n                # teilweises Kopieren f√ºr Linear Layer\n                if len(v.shape) == 2 and len(tgt_sd[k].shape) == 2:\n                    # M√∂gliche minimale Dimension ermitteln\n                    min_rows = min(v.shape[0], tgt_sd[k].shape[0])\n                    min_cols = min(v.shape[1], tgt_sd[k].shape[1])\n                    tgt_sd[k][:min_rows, :min_cols] = v[:min_rows, :min_cols]\n                # Bias teilweise √ºbernehmen\n                elif len(v.shape) == 1 and len(tgt_sd[k].shape) == 1:\n                    min_len = min(len(v), len(tgt_sd[k]))\n                    tgt_sd[k][:min_len] = v[:min_len]\n    target.load_state_dict(tgt_sd, strict=False)\n\ndef create_child_from_parents(parent_agent, mutation_factor=5, epsilon=1, min_mutations=1):\n    n_mutations = max(min_mutations, int(mutation_factor * epsilon))\n    new_layers = mutate_architecture(get_layer_info(parent_agent), n_mutations=n_mutations)\n    child_agent = DQNAgent(\n        state_dim=state_size,\n        action_dim=number_actions,\n        learning_rate=learning_rate,\n        buffer_size=buffer_size,\n        batch_size=batch_size,\n        gamma=gamma,\n        tau=tau,\n        layer_info=new_layers\n    )\n\n    # Gewichte vom Elternteil √ºbernehmen, soweit m√∂glich\n    partial_load_state_dict_flexible(child_agent.q_network, parent_agent.q_network.state_dict())\n    \n    return PopulationMember(child_agent)\n\ndef check_architecture_dominance(win_counter, threshold=6):\n    for arch, count in win_counter.items():\n        if count >= threshold:\n            return list(arch)\n    return None\n\ndef remove_duplicate_models(population):\n    \"\"\"\n    Entfernt Population-Mitglieder mit identischer Layer-Architektur.\n    Behalte nur das erste Vorkommen jeder Architektur.\n    \"\"\"\n    seen_architectures = set()\n    unique_population = []\n\n    for member in population:\n        arch_tuple = tuple(get_layer_info(member.agent))\n        if arch_tuple not in seen_architectures:\n            seen_architectures.add(arch_tuple)\n            unique_population.append(member)\n\n    return unique_population","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.866915Z","iopub.execute_input":"2026-01-10T15:16:55.867307Z","iopub.status.idle":"2026-01-10T15:16:55.886003Z","shell.execute_reply.started":"2026-01-10T15:16:55.867274Z","shell.execute_reply":"2026-01-10T15:16:55.885024Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"def train_agent_for_episodes(\n    env,\n    agent,\n    num_episodes,\n    generation,\n    max_steps_per_episode,\n    epsilon_start,\n    epsilon_end,\n    epsilon_decay,\n):\n    \"\"\"\n    Trainiert einen Agenten f√ºr num_episodes Episoden.\n    Œµ ist strikt lokal.\n    Gibt Mean-Reward zur√ºck.\n    \"\"\"\n    epsilon = epsilon_start\n    rewards = []\n\n    for ep in range(num_episodes):\n        reset_out = env.reset(seed=generation * 10_000 + ep)\n        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n        episode_reward = 0.0\n\n        for _ in range(max_steps_per_episode):\n            action = agent.select_action(state, epsilon)\n\n            step_out = env.step(action)\n            if len(step_out) == 5:\n                next_state, reward, terminated, truncated, _ = step_out\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = step_out\n\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                break\n\n        rewards.append(episode_reward)\n\n        # Œµ nur lokal decayn\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n\n    return float(np.mean(rewards))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.887347Z","iopub.execute_input":"2026-01-10T15:16:55.887676Z","iopub.status.idle":"2026-01-10T15:16:55.910543Z","shell.execute_reply.started":"2026-01-10T15:16:55.887651Z","shell.execute_reply":"2026-01-10T15:16:55.909547Z"}},"outputs":[],"execution_count":114},{"cell_type":"code","source":"def run_trained_model_and_record(\n    model_dir,\n    env_name=\"CartPole-v1\",\n    video_dir=\"videos\",\n    max_steps=500,\n    device=\"cpu\"\n):\n\n    os.makedirs(video_dir, exist_ok=True)\n\n    # --------\n    # Modell + Metadaten laden\n    # --------\n    model, meta = load_model_bundle(model_dir, device=device)\n    model.eval()\n    print(f\"Lade Modell aus '{model_dir}' mit Layer-Info: {meta['layer_info']}\")\n\n    # --------\n    # ENV vorbereiten + Video-Wrapper\n    # --------\n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    env = RecordVideo(\n        env,\n        video_folder=video_dir,\n        episode_trigger=lambda ep: True,  # jedes Episode aufnehmen\n        name_prefix=\"episode\"\n    )\n\n    reset_out = env.reset()\n    state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n    total_reward = 0\n\n    # --------\n    # Episode ausf√ºhren\n    # --------\n    for step in range(max_steps):\n        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            q_values = model(state_t)\n            action = torch.argmax(q_values, dim=-1).item()\n\n        step_out = env.step(action)\n        if len(step_out) == 5:\n            state, reward, terminated, truncated, _ = step_out\n            done = terminated or truncated\n        else:\n            state, reward, done, _ = step_out\n\n        total_reward += reward\n        if done:\n            break\n\n    env.close()\n\n    print(f\"Episode L√§nge: {step+1}\")\n    print(f\"Return: {total_reward}\")\n    print(f\"Video gespeichert in: {video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.911692Z","iopub.execute_input":"2026-01-10T15:16:55.912034Z","iopub.status.idle":"2026-01-10T15:16:55.934722Z","shell.execute_reply.started":"2026-01-10T15:16:55.912000Z","shell.execute_reply":"2026-01-10T15:16:55.933487Z"}},"outputs":[],"execution_count":115},{"cell_type":"code","source":"learning_rate = 5e-4\nbuffer_size = 100000\nbatch_size = 100\ngamma = 0.99\ntau = 1e-3\n\n# -----------------------------\n# Evolution√§re Trainingsparameter\n# -----------------------------\n\npopulation_size = 10\nmutation_factor = 5\nthresholddom = 5\nmin_layers=1 \nmax_layers=5\n\nmax_steps_per_episode = 1000\ngoal = 200.0\n\nepsilon_start = 1.0\nepsilon_end = 0.01\nepsilon_decay = 0.995\nscores_window = deque(maxlen=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.935966Z","iopub.execute_input":"2026-01-10T15:16:55.936250Z","iopub.status.idle":"2026-01-10T15:16:55.955984Z","shell.execute_reply.started":"2026-01-10T15:16:55.936225Z","shell.execute_reply":"2026-01-10T15:16:55.955165Z"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":"population = create_initial_population(population_size, state_dim=state_size, action_dim=number_actions)\narchitecture_win_counter = defaultdict(int)\ndominant_architecture = None\n\n# -----------------------------\n# Ordner + Log initialisieren\n# -----------------------------\nrun_dir = init_training_run(env_name)\nlog_csv = init_training_log(run_dir)\n\n# -----------------------------\n# Trainingsloop\n# -----------------------------\ngeneration = 0\nepsilon = epsilon_start\n\n# =========================\n# üîπ Population trainieren\n# =========================\nwhile True:\n    for member in population:\n        mean_score = train_agent_for_episodes(\n            env=env,\n            agent=member.agent,\n            num_episodes=batch_size,\n            generation=generation,\n            max_steps_per_episode=max_steps_per_episode,\n            epsilon_start=epsilon,\n            epsilon_end=epsilon_end,\n            epsilon_decay=epsilon_decay,\n        )\n        member.total_reward = mean_score\n        print(f\"training von {get_layer_info(member.agent)} fertig\")\n        \n    population.sort(key=lambda m: m.total_reward, reverse=True)\n    print(f\"\\nüèÖ Population Ranking ‚Äì Generation {generation}\")\n    for rank, member in enumerate(population, start=1):\n        print(\n            f\"{rank:2d}. Reward: {member.total_reward:8.2f} | \"\n            f\"Layers: {get_layer_info(member.agent)}\"\n        )\n        append_generation_log(\n            csv_path=log_csv,\n            env_name=env_name,\n            generation=generation,\n            hidden_sizes=get_layer_info(member.agent),\n            fitness=member.total_reward,\n            episodes_per_individual=batch_size\n        )\n    \n    if population[0].total_reward >= goal:\n        # üîπ Nur das beste Modell speichern\n        save_dir = os.path.join(run_dir, f\"checkpoint_generation_{generation}\")\n        save_model_bundle(\n            model=population[0].agent.q_network,\n            save_dir=save_dir,\n            state_dim=state_size,\n            action_dim=number_actions,\n            fitness=population[0].total_reward,\n            generation=generation\n        )\n        print(f\"\\nüéØ Goal erreicht mit {get_layer_info(population[0].agent)} in Generation {generation}\")\n        break\n    \n    # Dominanz pr√ºfen und +\n    architecture_win_counter[tuple(get_layer_info(population[0].agent))] += 1\n    dominant_architecture = check_architecture_dominance(architecture_win_counter, thresholddom)\n    \n    # üîπ Neue Population erzeugen\n    new_population = []\n    if dominant_architecture:\n        new_population = [population[0]]  # nur dominant weitertrainieren\n    else:\n        new_population = [population[0], population[1]]\n        remaining_slots = population_size - 2\n        n_children_p1 = int(remaining_slots * 0.5)\n        n_children_p2 = remaining_slots - n_children_p1\n\n        print(f\"Kinder haben {max(1, int(mutation_factor * epsilon))} mutationen\")\n    \n        for _ in range(n_children_p1):\n            child = create_child_from_parents(population[0].agent, mutation_factor, epsilon)\n            new_population.append(child)\n        for _ in range(n_children_p2):\n            child = create_child_from_parents(population[1].agent, mutation_factor, epsilon)\n            new_population.append(child)\n\n    # üîπ Doppelte Modelle entfernen\n    new_population = remove_duplicate_models(new_population)\n    population = new_population\n    \n    # üîπ Epsilon decay\n    epsilon = max(epsilon_end, epsilon * (epsilon_decay ** batch_size))\n    generation += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:16:55.958619Z","iopub.execute_input":"2026-01-10T15:16:55.959377Z","iopub.status.idle":"2026-01-10T15:31:28.887233Z","shell.execute_reply.started":"2026-01-10T15:16:55.959342Z","shell.execute_reply":"2026-01-10T15:31:28.886135Z"}},"outputs":[{"name":"stdout","text":"üìÅ Neuer Trainingslauf: training_history/LunarLander-v3_2026-01-10_15-16-56\nüìä Training-Log erstellt: training_history/LunarLander-v3_2026-01-10_15-16-56/training_log.csv\ntraining von [104, 120] fertig\ntraining von [128, 24, 8, 80, 96, 48] fertig\ntraining von [112, 40, 112, 120, 72, 48] fertig\ntraining von [64, 24, 8, 72, 16] fertig\ntraining von [24, 80, 48, 56, 64, 16, 56, 112] fertig\ntraining von [88, 128, 88, 48, 48, 88] fertig\ntraining von [48, 112, 72] fertig\ntraining von [80, 32, 8, 120, 96, 72, 24, 40] fertig\ntraining von [88, 112, 16, 128, 8, 120, 104, 72, 120] fertig\ntraining von [80, 32, 72, 120, 24, 72, 120] fertig\n\nüèÖ Population Ranking ‚Äì Generation 0\n 1. Reward:  -155.99 | Layers: [48, 112, 72]\n 2. Reward:  -175.97 | Layers: [104, 120]\n 3. Reward:  -178.32 | Layers: [88, 128, 88, 48, 48, 88]\n 4. Reward:  -184.05 | Layers: [112, 40, 112, 120, 72, 48]\n 5. Reward:  -197.04 | Layers: [80, 32, 8, 120, 96, 72, 24, 40]\n 6. Reward:  -211.77 | Layers: [88, 112, 16, 128, 8, 120, 104, 72, 120]\n 7. Reward:  -217.15 | Layers: [64, 24, 8, 72, 16]\n 8. Reward:  -232.92 | Layers: [128, 24, 8, 80, 96, 48]\n 9. Reward:  -250.84 | Layers: [80, 32, 72, 120, 24, 72, 120]\n10. Reward:  -254.47 | Layers: [24, 80, 48, 56, 64, 16, 56, 112]\nKinder haben 5 mutationen\ntraining von [48, 112, 72] fertig\ntraining von [104, 120] fertig\ntraining von [24, 128] fertig\ntraining von [96, 96, 48, 112, 72] fertig\ntraining von [48, 56, 72] fertig\ntraining von [56, 112, 72, 72, 72] fertig\ntraining von [128, 128] fertig\ntraining von [26] fertig\ntraining von [64] fertig\ntraining von [104, 128, 128, 120] fertig\n\nüèÖ Population Ranking ‚Äì Generation 1\n 1. Reward:   -96.30 | Layers: [104, 120]\n 2. Reward:   -99.63 | Layers: [48, 56, 72]\n 3. Reward:  -112.66 | Layers: [48, 112, 72]\n 4. Reward:  -130.62 | Layers: [56, 112, 72, 72, 72]\n 5. Reward:  -132.27 | Layers: [96, 96, 48, 112, 72]\n 6. Reward:  -142.11 | Layers: [104, 128, 128, 120]\n 7. Reward:  -152.75 | Layers: [128, 128]\n 8. Reward:  -188.58 | Layers: [24, 128]\n 9. Reward:  -202.66 | Layers: [64]\n10. Reward:  -205.17 | Layers: [26]\nKinder haben 3 mutationen\ntraining von [104, 120] fertig\ntraining von [48, 56, 72] fertig\ntraining von [120] fertig\ntraining von [128] fertig\ntraining von [104] fertig\ntraining von [128, 120] fertig\ntraining von [48, 48, 128] fertig\ntraining von [24, 56, 72, 36] fertig\ntraining von [28] fertig\ntraining von [96, 28] fertig\n\nüèÖ Population Ranking ‚Äì Generation 2\n 1. Reward:   -40.56 | Layers: [48, 56, 72]\n 2. Reward:   -96.70 | Layers: [104, 120]\n 3. Reward:  -108.70 | Layers: [128, 120]\n 4. Reward:  -189.28 | Layers: [24, 56, 72, 36]\n 5. Reward:  -203.11 | Layers: [96, 28]\n 6. Reward:  -204.27 | Layers: [120]\n 7. Reward:  -205.65 | Layers: [48, 48, 128]\n 8. Reward:  -212.66 | Layers: [28]\n 9. Reward:  -235.54 | Layers: [104]\n10. Reward:  -243.96 | Layers: [128]\nKinder haben 1 mutationen\ntraining von [48, 56, 72] fertig\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_56/3359507439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmember\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         mean_score = train_agent_for_episodes(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_56/2302803729.py\u001b[0m in \u001b[0;36mtrain_agent_for_episodes\u001b[0;34m(env, agent, num_episodes, generation, max_steps_per_episode, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_56/2212364767.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_56/990897881.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, experience)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":117},{"cell_type":"code","source":"# Angenommen, das Modell wurde gespeichert in:\nmodel_dir = \"training_history/LunarLander-v3_2026-01-10_12-12-52/checkpoint_generation_16\"\n\n# Video ausf√ºhren und speichern\nrun_trained_model_and_record(\n    model_dir=model_dir,\n    env_name=env_name,                 # aus deinem Setup\n    video_dir=\"videos\",                # Ordner f√ºr Video\n    max_steps=max_steps_per_episode,   # aus deinem Setup\n    device=\"cpu\"                       # falls GPU nicht verf√ºgbar\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:31:28.888272Z","iopub.status.idle":"2026-01-10T15:31:28.888556Z","shell.execute_reply.started":"2026-01-10T15:31:28.888428Z","shell.execute_reply":"2026-01-10T15:31:28.888443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf checkpoints/\n# !rm -rf training_history/\n# !rm -rf videos/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:31:39.701530Z","iopub.execute_input":"2026-01-10T15:31:39.701876Z","iopub.status.idle":"2026-01-10T15:31:40.104160Z","shell.execute_reply.started":"2026-01-10T15:31:39.701848Z","shell.execute_reply":"2026-01-10T15:31:40.102968Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}