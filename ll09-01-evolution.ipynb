{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gymnasium\n# !pip install \"gymnasium[atari, accept-rom-license]\"\n# !apt-get install -y swig\n# !pip install gymnasium[box2d]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.212683Z","iopub.execute_input":"2026-01-10T12:12:48.213623Z","iopub.status.idle":"2026-01-10T12:12:48.217041Z","shell.execute_reply.started":"2026-01-10T12:12:48.213583Z","shell.execute_reply":"2026-01-10T12:12:48.216259Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ===========================\n# IMPORTS & GLOBAL SETUP\n# ===========================\nimport os\nimport json\nimport csv\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nfrom collections import deque\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport torch.nn.functional as F\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.218663Z","iopub.execute_input":"2026-01-10T12:12:48.218912Z","iopub.status.idle":"2026-01-10T12:12:48.234487Z","shell.execute_reply.started":"2026-01-10T12:12:48.218891Z","shell.execute_reply":"2026-01-10T12:12:48.233706Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import warnings\nimport logging\n\n# -----------------------\n# WARNINGS UNTERDRÃœCKEN\n# -----------------------\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Gym / MoviePy Logging leiser machen\nlogging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\nlogging.getLogger(\"moviepy\").setLevel(logging.ERROR)\n\n# SDL / Pygame Headless Fix (verhindert XDG_RUNTIME_DIR Meldung)\nos.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.235465Z","iopub.execute_input":"2026-01-10T12:12:48.235784Z","iopub.status.idle":"2026-01-10T12:12:48.253327Z","shell.execute_reply.started":"2026-01-10T12:12:48.235752Z","shell.execute_reply":"2026-01-10T12:12:48.252317Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"env_name = \"LunarLander-v3\"\nenv = gym.make(env_name)\nstate_shape = env.observation_space.shape\nstate_size = env.observation_space.shape[0]\nnumber_actions = env.action_space.n\nprint('State shape: ', state_shape)\nprint('State size: ', state_size)\nprint('Number of actions: ', number_actions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.254535Z","iopub.execute_input":"2026-01-10T12:12:48.254917Z","iopub.status.idle":"2026-01-10T12:12:48.724916Z","shell.execute_reply.started":"2026-01-10T12:12:48.254885Z","shell.execute_reply":"2026-01-10T12:12:48.724211Z"}},"outputs":[{"name":"stdout","text":"State shape:  (8,)\nState size:  8\nNumber of actions:  4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    \"\"\"\n    Dynamisches MLP fÃ¼r DQN.\n    Linear + ReLU nur zwischen Hidden-Layern\n    \"\"\"\n    def __init__(self, state_dim, action_dim, layer_sizes=[64, 8, 64, 64]):\n        super().__init__()\n        torch.manual_seed(42)\n\n        layers = []\n        input_size = state_dim\n        for i, size in enumerate(layer_sizes):\n            layers.append(nn.Linear(input_size, size))\n            # ReLU nur nach Hidden-Layern, nicht nach dem letzten Layer\n            if i != len(layer_sizes) - 1:\n                layers.append(nn.ReLU())\n            input_size = size\n\n        # Output Layer\n        layers.append(nn.Linear(input_size, action_dim))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.726742Z","iopub.execute_input":"2026-01-10T12:12:48.727100Z","iopub.status.idle":"2026-01-10T12:12:48.733660Z","shell.execute_reply.started":"2026-01-10T12:12:48.727068Z","shell.execute_reply":"2026-01-10T12:12:48.732859Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class ReplayBuffer:\n    \"\"\"\n    Speichert vergangene Erfahrungen:\n    (state, action, reward, next_state, done)\n\n    Warum?\n    - bricht zeitliche Korrelationen\n    - alte Erfahrungen kÃ¶nnen mehrfach gelernt werden\n    \"\"\"\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, experience):\n        \"\"\"\n        FÃ¼gt eine Erfahrung hinzu.\n        Wenn der Speicher voll ist â†’ die Ã¤lteste fliegt raus.\n        \"\"\"\n        self.memory.append(experience)\n        if len(self.memory) > self.capacity:\n            self.memory.pop(0)\n\n    def sample(self, batch_size):\n        \"\"\"\n        Zieht zufÃ¤llige Erfahrungen fÃ¼r ein Lern-Update\n        \"\"\"\n        batch = random.sample(self.memory, batch_size)\n\n        states      = torch.tensor(np.vstack([e[0] for e in batch]), dtype=torch.float32).to(self.device)\n        actions     = torch.tensor(np.vstack([e[1] for e in batch]), dtype=torch.long).to(self.device)\n        rewards     = torch.tensor(np.vstack([e[2] for e in batch]), dtype=torch.float32).to(self.device)\n        next_states = torch.tensor(np.vstack([e[3] for e in batch]), dtype=torch.float32).to(self.device)\n        dones       = torch.tensor(np.vstack([e[4] for e in batch]), dtype=torch.float32).to(self.device)\n\n        return states, actions, rewards, next_states, dones\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.734701Z","iopub.execute_input":"2026-01-10T12:12:48.735387Z","iopub.status.idle":"2026-01-10T12:12:48.753925Z","shell.execute_reply.started":"2026-01-10T12:12:48.735358Z","shell.execute_reply":"2026-01-10T12:12:48.753107Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class DQNAgent:\n    \"\"\"\n    DQN-Agent mit dynamischem QNetwork (beliebige Layer-Architektur)\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        learning_rate,\n        buffer_size,\n        batch_size,\n        gamma,\n        tau,\n        layer_info=None,  # <-- Neue Option fÃ¼r dynamische Architekturen\n    ):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.batch_size = batch_size\n        self.gamma = gamma          # Discount-Faktor\n        self.tau = tau              # Soft-Update-Faktor bzw interpolation faktor\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.seed = SEED\n\n        # Standard-Layer falls nichts angegeben\n        if layer_info is None:\n            layer_info = [64, 64, 64]\n\n        # Online-Netzwerk\n        self.q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n\n        # Target-Netzwerk\n        self.target_q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n        self.target_q_network.load_state_dict(self.q_network.state_dict())  # initial synchronisieren\n\n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n        # Replay Buffer\n        self.replay_buffer = ReplayBuffer(buffer_size)\n\n        self.step_counter = 0\n\n    def select_action(self, state, epsilon = 0.):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        self.q_network.eval()\n        with torch.no_grad():\n            q_values = self.q_network(state)\n        self.q_network.train()\n\n        if random.random() > epsilon:\n            return q_values.argmax(dim=1).item()\n        else:\n            return random.randrange(self.action_dim)\n\n    def step(self, state, action, reward, next_state, done):\n        self.replay_buffer.add((state, action, reward, next_state, done))\n        self.step_counter += 1\n\n        # Nicht bei jedem Schritt lernen (stabiler)\n        if self.step_counter % 4 == 0:\n            if len(self.replay_buffer.memory) >= self.batch_size:\n                batch = self.replay_buffer.sample(self.batch_size)\n                self.learn(batch)\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states, dones = experiences\n\n        with torch.no_grad():\n            next_q_values = self.target_q_network(next_states).max(dim=1, keepdim=True)[0]\n            q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n\n        q_expected = self.q_network(states).gather(1, actions)\n\n        loss = F.mse_loss(q_expected, q_targets)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.soft_update()\n\n    def soft_update(self):\n        for target_param, local_param in zip(\n            self.target_q_network.parameters(),\n            self.q_network.parameters()\n        ):\n            target_param.data.copy_(\n                self.tau * local_param.data +\n                (1.0 - self.tau) * target_param.data\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.755340Z","iopub.execute_input":"2026-01-10T12:12:48.755622Z","iopub.status.idle":"2026-01-10T12:12:48.771083Z","shell.execute_reply.started":"2026-01-10T12:12:48.755597Z","shell.execute_reply":"2026-01-10T12:12:48.770251Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_layer_info(agent_or_model):\n    \"\"\"\n    Gibt nur die GrÃ¶ÃŸe der Linear-Layer zurÃ¼ck, z.B. [64, 64]\n    \"\"\"\n    if hasattr(agent_or_model, \"q_network\"):\n        model = agent_or_model.q_network\n    else:\n        model = agent_or_model\n\n    layer_sizes = []\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear) and layer.out_features != model.model[-1].out_features:\n            # Letzter Linear-Layer (Output) ignorieren\n            layer_sizes.append(layer.out_features)\n    return layer_sizes\n\n# -------------------------------\n# load_model_bundle: rekonstruiert Linear+ReLU Modell\n# -------------------------------\ndef save_model_bundle(model, save_dir, state_dim, action_dim, fitness=None, generation=None):\n    os.makedirs(save_dir, exist_ok=True)\n    # 1ï¸âƒ£ Weights speichern\n    torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n\n    # 2ï¸âƒ£ Meta speichern\n    meta = {\n        \"state_dim\": int(state_dim),\n        \"action_dim\": int(action_dim),\n        \"layer_info\": get_layer_info(model),\n        \"fitness\": float(fitness) if fitness is not None else None,\n        \"generation\": int(generation) if generation is not None else None,\n        \"seed\": int(SEED) if SEED is not None else None,\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    with open(os.path.join(save_dir, \"meta.json\"), \"w\") as f:\n        json.dump(meta, f, indent=4)\n    print(f\"âœ“ Model saved to {save_dir}\")\n\ndef load_model_bundle(model_dir, device=\"cpu\"):\n    # Meta laden\n    with open(os.path.join(model_dir, \"meta.json\"), \"r\") as f:\n        meta = json.load(f)\n    state_dim = meta[\"state_dim\"]\n    action_dim = meta[\"action_dim\"]\n    layer_info = meta[\"layer_info\"]\n\n    # Modell rekonstruieren\n    model = QNetwork(state_dim, action_dim, layer_info).to(device)\n\n    # Gewichte laden (reihenfolgebasiert)\n    state_dict_saved = torch.load(os.path.join(model_dir, \"model.pth\"), map_location=device)\n    state_dict_model = model.state_dict()\n    for k_model, k_saved in zip(state_dict_model.keys(), state_dict_saved.keys()):\n        state_dict_model[k_model] = state_dict_saved[k_saved]\n    model.load_state_dict(state_dict_model)\n\n    return model, meta\n\n\ndef init_training_run(env_name, base_dir=\"training_history\"):\n    \"\"\"\n    Erstellt einen neuen Run-Ordner mit Timestamp\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    run_dir = os.path.join(base_dir, f\"{env_name}_{timestamp}\")\n    os.makedirs(run_dir, exist_ok=True)\n\n    print(f\"ğŸ“ Neuer Trainingslauf: {run_dir}\")\n    return run_dir\n\ndef init_training_log(run_dir):\n    \"\"\"\n    Erstellt CSV-Logdatei fÃ¼r einen Trainingslauf\n    \"\"\"\n    csv_path = os.path.join(run_dir, \"training_log.csv\")\n\n    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n        writer.writerow([\n            \"Environment\",\n            \"Generation\",\n            \"Model_Architecture\",\n            \"Fitness\",\n            \"Trained_Episodes\",\n        ])\n\n    print(f\"ğŸ“Š Training-Log erstellt: {csv_path}\")\n    return csv_path\n    \ndef append_generation_log(\n    csv_path,\n    env_name,\n    generation,\n    hidden_sizes,\n    fitness,\n    episodes_per_individual,\n):\n    with open(csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n\n        writer.writerow([\n            env_name,\n            generation,\n            str(hidden_sizes),\n            f\"{fitness:.2f}\",\n            generation * episodes_per_individual,\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.772247Z","iopub.execute_input":"2026-01-10T12:12:48.772518Z","iopub.status.idle":"2026-01-10T12:12:48.792541Z","shell.execute_reply.started":"2026-01-10T12:12:48.772494Z","shell.execute_reply":"2026-01-10T12:12:48.791629Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class PopulationMember:\n    def __init__(self, agent):\n        self.agent = agent\n        self.total_reward = 0.0\n        self.episode_rewards = []\n\ndef select_parents(sorted_population):\n    return sorted_population[0], sorted_population[1]\n\ndef check_architecture_dominance(\n    architecture_win_counter,\n    threshold=6,\n):\n    for arch, wins in architecture_win_counter.items():\n        if wins >= threshold:\n            return list(arch)\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.793568Z","iopub.execute_input":"2026-01-10T12:12:48.793841Z","iopub.status.idle":"2026-01-10T12:12:48.810610Z","shell.execute_reply.started":"2026-01-10T12:12:48.793818Z","shell.execute_reply":"2026-01-10T12:12:48.809753Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# =========================\n# ğŸš€ MUTATION\n# =========================\ndef mutate_architecture(\n    layer_info, \n    n_mutations=1, \n    min_neurons=8, \n    max_neurons=128\n):\n    \"\"\"\n    Mutiert ein Layer-Array n_mutations Mal.\n    Optionen pro Mutation:\n    1ï¸âƒ£ Dupliziere oder lÃ¶sche eine zufÃ¤llige Schicht (Layer-Anzahl begrenzt auf min_layers/max_layers)\n    2ï¸âƒ£ Neuronen *2 oder /2 (begrenzt auf min_neurons/max_neurons)\n    \"\"\"\n    new_layers = layer_info.copy()\n    for _ in range(n_mutations):\n        if len(new_layers) == 0:\n            # Falls alle Layer gelÃ¶scht wurden, mindestens 1 Layer wieder hinzufÃ¼gen\n            new_layers.append(min_neurons)\n            continue\n\n        idx = random.randint(0, len(new_layers)-1)\n        op = random.choice([\"layer_op\", \"scale\"])\n\n        if op == \"layer_op\":\n            # Duplizieren oder lÃ¶schen, Layer-Anzahl beachten\n            if len(new_layers) == 1:\n                # Nur duplizieren mÃ¶glich\n                if len(new_layers) < max_layers:\n                    new_layers.insert(idx, new_layers[idx])\n            else:\n                if random.random() < 0.5:\n                    # duplizieren (nur wenn max_layers nicht Ã¼berschritten)\n                    if len(new_layers) < max_layers:\n                        new_layers.insert(idx, new_layers[idx])\n                else:\n                    # lÃ¶schen (nur wenn min_layers nicht unterschritten)\n                    if len(new_layers) > min_layers:\n                        new_layers.pop(idx)\n        else:\n            # Neuronen *2 oder /2\n            factor = random.choice([0.5, 2])\n            new_layers[idx] = max(min_neurons, min(max_neurons, int(new_layers[idx] * factor)))\n\n    return new_layers\n\n\ndef create_initial_population(population_size, state_dim, action_dim, min_layers=1, max_layers=10,\n                              min_neurons=8, max_neurons=128):\n    \"\"\"\n    Initialisiert Population. Layer-GrÃ¶ÃŸe als Faktor von min_neurons.\n    \"\"\"\n    population = []\n    for _ in range(population_size):\n        num_layers = random.randint(min_layers, max_layers)\n        layers = [random.randint(1, max_neurons // min_neurons) * min_neurons for _ in range(num_layers)]\n        agent = DQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            learning_rate=learning_rate,\n            buffer_size=buffer_size,\n            batch_size=batch_size,\n            gamma=gamma,\n            tau=tau,\n            layer_info=layers\n        )\n        population.append(PopulationMember(agent))\n    return population\n\ndef create_child_from_parents(parent_agent, mutation_factor=5, epsilon=1, min_mutations=1):\n    n_mutations = max(min_mutations, int(mutation_factor * epsilon))\n    new_layers = mutate_architecture(get_layer_info(parent_agent), n_mutations=n_mutations)\n    child_agent = DQNAgent(\n        state_dim=state_size,\n        action_dim=number_actions,\n        learning_rate=learning_rate,\n        buffer_size=buffer_size,\n        batch_size=batch_size,\n        gamma=gamma,\n        tau=tau,\n        layer_info=new_layers\n    )\n    return PopulationMember(child_agent)\n\ndef check_architecture_dominance(win_counter, threshold=6):\n    for arch, count in win_counter.items():\n        if count >= threshold:\n            return list(arch)\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.811672Z","iopub.execute_input":"2026-01-10T12:12:48.811940Z","iopub.status.idle":"2026-01-10T12:12:48.828511Z","shell.execute_reply.started":"2026-01-10T12:12:48.811915Z","shell.execute_reply":"2026-01-10T12:12:48.827803Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_agent_for_episodes(\n    env,\n    agent,\n    num_episodes,\n    generation,\n    max_steps_per_episode,\n    epsilon_start,\n    epsilon_end,\n    epsilon_decay,\n):\n    \"\"\"\n    Trainiert einen Agenten fÃ¼r num_episodes Episoden.\n    Îµ ist strikt lokal.\n    Gibt Mean-Reward zurÃ¼ck.\n    \"\"\"\n    epsilon = epsilon_start\n    rewards = []\n\n    for ep in range(num_episodes):\n        reset_out = env.reset(seed=generation * 10_000 + ep)\n        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n        episode_reward = 0.0\n\n        for _ in range(max_steps_per_episode):\n            action = agent.select_action(state, epsilon)\n\n            step_out = env.step(action)\n            if len(step_out) == 5:\n                next_state, reward, terminated, truncated, _ = step_out\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = step_out\n\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                break\n\n        rewards.append(episode_reward)\n\n        # Îµ nur lokal decayn\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n\n    return float(np.mean(rewards))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.829524Z","iopub.execute_input":"2026-01-10T12:12:48.829807Z","iopub.status.idle":"2026-01-10T12:12:48.849012Z","shell.execute_reply.started":"2026-01-10T12:12:48.829774Z","shell.execute_reply":"2026-01-10T12:12:48.848213Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def run_trained_model_and_record(\n    model_dir,\n    env_name=\"CartPole-v1\",\n    video_dir=\"videos\",\n    max_steps=500,\n    device=\"cpu\"\n):\n\n    os.makedirs(video_dir, exist_ok=True)\n\n    # --------\n    # Modell + Metadaten laden\n    # --------\n    model, meta = load_model_bundle(model_dir, device=device)\n    model.eval()\n    print(f\"Lade Modell aus '{model_dir}' mit Layer-Info: {meta['layer_info']}\")\n\n    # --------\n    # ENV vorbereiten + Video-Wrapper\n    # --------\n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    env = RecordVideo(\n        env,\n        video_folder=video_dir,\n        episode_trigger=lambda ep: True,  # jedes Episode aufnehmen\n        name_prefix=\"episode\"\n    )\n\n    reset_out = env.reset()\n    state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n    total_reward = 0\n\n    # --------\n    # Episode ausfÃ¼hren\n    # --------\n    for step in range(max_steps):\n        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            q_values = model(state_t)\n            action = torch.argmax(q_values, dim=-1).item()\n\n        step_out = env.step(action)\n        if len(step_out) == 5:\n            state, reward, terminated, truncated, _ = step_out\n            done = terminated or truncated\n        else:\n            state, reward, done, _ = step_out\n\n        total_reward += reward\n        if done:\n            break\n\n    env.close()\n\n    print(f\"Episode LÃ¤nge: {step+1}\")\n    print(f\"Return: {total_reward}\")\n    print(f\"Video gespeichert in: {video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.850067Z","iopub.execute_input":"2026-01-10T12:12:48.850442Z","iopub.status.idle":"2026-01-10T12:12:48.868418Z","shell.execute_reply.started":"2026-01-10T12:12:48.850409Z","shell.execute_reply":"2026-01-10T12:12:48.867567Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"learning_rate = 5e-4\nbuffer_size = int(1e5)\nbatch_size = 50\ngamma = 0.99\ntau = 1e-3\n\n# -----------------------------\n# EvolutionÃ¤re Trainingsparameter\n# -----------------------------\n\npopulation_size = 10\nmutation_factor = 5\nthresholddom = 5\nmin_layers=1 \nmax_layers=5\n\nmax_steps_per_episode = 1000\ngoal = 200.0\n\nepsilon_start = 1.0\nepsilon_end = 0.01\nepsilon_decay = 0.995\nscores_window = deque(maxlen=100)\n\npopulation = create_initial_population(population_size, state_dim=state_size, action_dim=number_actions)\narchitecture_win_counter = defaultdict(int)\ndominant_architecture = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:48.869621Z","iopub.execute_input":"2026-01-10T12:12:48.869971Z","iopub.status.idle":"2026-01-10T12:12:52.385169Z","shell.execute_reply.started":"2026-01-10T12:12:48.869938Z","shell.execute_reply":"2026-01-10T12:12:52.384378Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# -----------------------------\n# Ordner + Log initialisieren\n# -----------------------------\nrun_dir = init_training_run(env_name)\nlog_csv = init_training_log(run_dir)\n\n# -----------------------------\n# Trainingsloop\n# -----------------------------\ngeneration = 0\nepsilon = epsilon_start\n\n# =========================\n# ğŸ”¹ Population trainieren\n# =========================\nwhile True:\n    for member in population:\n        mean_score = train_agent_for_episodes(\n            env=env,\n            agent=member.agent,\n            num_episodes=batch_size,\n            generation=generation,\n            max_steps_per_episode=max_steps_per_episode,\n            epsilon_start=epsilon,\n            epsilon_end=epsilon_end,\n            epsilon_decay=epsilon_decay,\n        )\n        member.total_reward = mean_score\n        print(f\"training von {get_layer_info(member.agent)} fertig\")\n        \n    population.sort(key=lambda m: m.total_reward, reverse=True)\n    print(f\"\\nğŸ… Population Ranking â€“ Generation {generation}\")\n    for rank, member in enumerate(population, start=1):\n        print(\n            f\"{rank:2d}. Reward: {member.total_reward:8.2f} | \"\n            f\"Layers: {get_layer_info(member.agent)}\"\n        )\n        append_generation_log(\n            csv_path=log_csv,\n            env_name=env_name,\n            generation=generation,\n            hidden_sizes=get_layer_info(member.agent),\n            fitness=member.total_reward,\n            episodes_per_individual=batch_size\n        )\n    \n    if population[0].total_reward >= goal:\n        # ğŸ”¹ Nur das beste Modell speichern\n        save_dir = os.path.join(run_dir, f\"checkpoint_generation_{generation}\")\n        save_model_bundle(\n            model=population[0].agent.q_network,\n            save_dir=save_dir,\n            state_dim=state_size,\n            action_dim=number_actions,\n            fitness=population[0].total_reward,\n            generation=generation\n        )\n        print(f\"\\nğŸ¯ Goal erreicht mit {get_layer_info(population[0].agent)} in Generation {generation}\")\n        break\n    \n    # Dominanz prÃ¼fen und +\n    architecture_win_counter[tuple(get_layer_info(population[0].agent))] += 1\n    dominant_architecture = check_architecture_dominance(architecture_win_counter, thresholddom)\n    \n    # ğŸ”¹ Neue Population erzeugen\n    new_population = []\n    if dominant_architecture:\n        new_population = [population[0]]  # nur dominant weitertrainieren\n    else:\n        new_population = [population[0], population[1]]\n        remaining_slots = population_size - 2\n        n_children_p1 = int(remaining_slots * 0.5)\n        n_children_p2 = remaining_slots - n_children_p1\n\n        print(f\"Kinder haben {max(1, int(mutation_factor * epsilon))} mutationen\")\n    \n        for _ in range(n_children_p1):\n            child = create_child_from_parents(population[0].agent, mutation_factor, epsilon)\n            new_population.append(child)\n        for _ in range(n_children_p2):\n            child = create_child_from_parents(population[1].agent, mutation_factor, epsilon)\n            new_population.append(child)\n    \n    population = new_population\n    \n    # ğŸ”¹ Epsilon decay\n    epsilon = max(epsilon_end, epsilon * (epsilon_decay ** batch_size))\n    generation += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:52.387434Z","iopub.execute_input":"2026-01-10T12:12:52.387835Z","iopub.status.idle":"2026-01-10T12:26:22.949385Z","shell.execute_reply.started":"2026-01-10T12:12:52.387808Z","shell.execute_reply":"2026-01-10T12:26:22.948474Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Neuer Trainingslauf: training_history/LunarLander-v3_2026-01-10_12-12-52\nğŸ“Š Training-Log erstellt: training_history/LunarLander-v3_2026-01-10_12-12-52/training_log.csv\ntraining von [96, 48, 128, 40, 72, 112] fertig\ntraining von [32, 24, 96, 40, 112] fertig\ntraining von [64, 16, 72, 104] fertig\ntraining von [16, 24, 120, 48, 72, 16, 104, 96, 56, 112] fertig\ntraining von [48, 8, 120] fertig\ntraining von [72, 8, 96, 112, 16, 40] fertig\ntraining von [128, 56, 16] fertig\ntraining von [16, 112] fertig\ntraining von [120, 120, 112, 80, 112, 8, 128, 72, 96] fertig\ntraining von [112, 48, 56, 88, 56, 104, 120] fertig\n\nğŸ… Population Ranking â€“ Generation 0\n 1. Reward:  -173.44 | Layers: [128, 56, 16]\n 2. Reward:  -197.89 | Layers: [96, 48, 128, 40, 72, 112]\n 3. Reward:  -207.69 | Layers: [32, 24, 96, 40, 112]\n 4. Reward:  -208.15 | Layers: [48, 8, 120]\n 5. Reward:  -208.42 | Layers: [120, 120, 112, 80, 112, 8, 128, 72, 96]\n 6. Reward:  -210.00 | Layers: [64, 16, 72, 104]\n 7. Reward:  -212.58 | Layers: [16, 112]\n 8. Reward:  -220.74 | Layers: [72, 8, 96, 112, 16, 40]\n 9. Reward:  -230.75 | Layers: [112, 48, 56, 88, 56, 104, 120]\n10. Reward:  -233.56 | Layers: [16, 24, 120, 48, 72, 16, 104, 96, 56, 112]\nKinder haben 5 mutationen\ntraining von [128, 56, 16] fertig\ntraining von [96, 48, 128, 40, 72, 112] fertig\ntraining von [128, 32, 32] fertig\ntraining von [56, 16] fertig\ntraining von [56, 56, 32] fertig\ntraining von [128, 56] fertig\ntraining von [96, 64, 128, 112] fertig\ntraining von [96, 40, 72, 112] fertig\ntraining von [96, 128, 40, 18] fertig\ntraining von [128, 40, 112] fertig\n\nğŸ… Population Ranking â€“ Generation 1\n 1. Reward:  -168.27 | Layers: [96, 128, 40, 18]\n 2. Reward:  -168.46 | Layers: [128, 56, 16]\n 3. Reward:  -174.29 | Layers: [56, 56, 32]\n 4. Reward:  -175.08 | Layers: [128, 40, 112]\n 5. Reward:  -177.06 | Layers: [128, 56]\n 6. Reward:  -178.20 | Layers: [56, 16]\n 7. Reward:  -206.75 | Layers: [96, 64, 128, 112]\n 8. Reward:  -215.05 | Layers: [96, 48, 128, 40, 72, 112]\n 9. Reward:  -215.07 | Layers: [128, 32, 32]\n10. Reward:  -216.69 | Layers: [96, 40, 72, 112]\nKinder haben 3 mutationen\ntraining von [96, 128, 40, 18] fertig\ntraining von [128, 56, 16] fertig\ntraining von [96, 64, 40, 18] fertig\ntraining von [64, 40, 36] fertig\ntraining von [96, 40, 36, 36] fertig\ntraining von [48, 48, 128, 40, 18] fertig\ntraining von [56, 8] fertig\ntraining von [112] fertig\ntraining von [16] fertig\ntraining von [32, 16] fertig\n\nğŸ… Population Ranking â€“ Generation 2\n 1. Reward:  -108.20 | Layers: [96, 128, 40, 18]\n 2. Reward:  -201.40 | Layers: [56, 8]\n 3. Reward:  -209.62 | Layers: [128, 56, 16]\n 4. Reward:  -229.66 | Layers: [48, 48, 128, 40, 18]\n 5. Reward:  -246.37 | Layers: [16]\n 6. Reward:  -250.43 | Layers: [32, 16]\n 7. Reward:  -256.58 | Layers: [112]\n 8. Reward:  -258.30 | Layers: [64, 40, 36]\n 9. Reward:  -276.31 | Layers: [96, 64, 40, 18]\n10. Reward:  -299.35 | Layers: [96, 40, 36, 36]\nKinder haben 3 mutationen\ntraining von [96, 128, 40, 18] fertig\ntraining von [56, 8] fertig\ntraining von [96, 64] fertig\ntraining von [128, 128] fertig\ntraining von [96, 128, 40, 40, 18] fertig\ntraining von [128, 128, 40, 18] fertig\ntraining von [16, 8] fertig\ntraining von [14, 8, 8] fertig\ntraining von [56, 8] fertig\ntraining von [8] fertig\n\nğŸ… Population Ranking â€“ Generation 3\n 1. Reward:  -116.38 | Layers: [96, 128, 40, 18]\n 2. Reward:  -173.72 | Layers: [56, 8]\n 3. Reward:  -227.65 | Layers: [96, 64]\n 4. Reward:  -243.70 | Layers: [128, 128]\n 5. Reward:  -252.63 | Layers: [128, 128, 40, 18]\n 6. Reward:  -264.42 | Layers: [8]\n 7. Reward:  -276.05 | Layers: [96, 128, 40, 40, 18]\n 8. Reward:  -320.55 | Layers: [56, 8]\n 9. Reward:  -360.05 | Layers: [14, 8, 8]\n10. Reward:  -380.15 | Layers: [16, 8]\nKinder haben 2 mutationen\ntraining von [96, 128, 40, 18] fertig\ntraining von [56, 8] fertig\ntraining von [96, 40] fertig\ntraining von [96, 128, 40, 9] fertig\ntraining von [96, 96, 128, 40, 36] fertig\ntraining von [96, 128, 40, 40, 18] fertig\ntraining von [56, 16] fertig\ntraining von [56] fertig\ntraining von [56, 8, 8] fertig\ntraining von [112, 8] fertig\n\nğŸ… Population Ranking â€“ Generation 4\n 1. Reward:  -103.57 | Layers: [96, 128, 40, 18]\n 2. Reward:  -185.71 | Layers: [56, 8]\n 3. Reward:  -203.17 | Layers: [96, 40]\n 4. Reward:  -228.38 | Layers: [56, 16]\n 5. Reward:  -237.21 | Layers: [56]\n 6. Reward:  -264.95 | Layers: [96, 128, 40, 9]\n 7. Reward:  -269.65 | Layers: [96, 128, 40, 40, 18]\n 8. Reward:  -275.13 | Layers: [96, 96, 128, 40, 36]\n 9. Reward:  -299.74 | Layers: [56, 8, 8]\n10. Reward:  -362.32 | Layers: [112, 8]\nKinder haben 1 mutationen\ntraining von [96, 128, 40, 18] fertig\ntraining von [56, 8] fertig\ntraining von [96, 128, 40, 40, 18] fertig\ntraining von [96, 128, 80, 18] fertig\ntraining von [96, 128, 40, 18, 18] fertig\ntraining von [128, 128, 40, 18] fertig\ntraining von [56, 16] fertig\ntraining von [28, 8] fertig\ntraining von [56, 8] fertig\ntraining von [56] fertig\n\nğŸ… Population Ranking â€“ Generation 5\n 1. Reward:   -43.60 | Layers: [96, 128, 40, 18]\n 2. Reward:  -161.75 | Layers: [56]\n 3. Reward:  -189.53 | Layers: [56, 8]\n 4. Reward:  -232.31 | Layers: [96, 128, 40, 40, 18]\n 5. Reward:  -236.47 | Layers: [96, 128, 40, 18, 18]\n 6. Reward:  -242.17 | Layers: [28, 8]\n 7. Reward:  -248.37 | Layers: [128, 128, 40, 18]\n 8. Reward:  -265.39 | Layers: [56, 8]\n 9. Reward:  -318.31 | Layers: [96, 128, 80, 18]\n10. Reward:  -319.59 | Layers: [56, 16]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 6\n 1. Reward:  -101.62 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 7\n 1. Reward:   -81.49 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 8\n 1. Reward:   -61.55 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 9\n 1. Reward:    -7.28 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 10\n 1. Reward:     0.21 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 11\n 1. Reward:   109.91 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 12\n 1. Reward:   198.12 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 13\n 1. Reward:   157.17 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 14\n 1. Reward:   186.35 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 15\n 1. Reward:   187.17 | Layers: [96, 128, 40, 18]\ntraining von [96, 128, 40, 18] fertig\n\nğŸ… Population Ranking â€“ Generation 16\n 1. Reward:   212.44 | Layers: [96, 128, 40, 18]\nâœ“ Model saved to training_history/LunarLander-v3_2026-01-10_12-12-52/checkpoint_generation_16\n\nğŸ¯ Goal erreicht mit [96, 128, 40, 18] in Generation 16\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Angenommen, das Modell wurde gespeichert in:\nmodel_dir = \"training_history/LunarLander-v3_2026-01-10_12-12-52/checkpoint_generation_16\"\n\n# Video ausfÃ¼hren und speichern\nrun_trained_model_and_record(\n    model_dir=model_dir,\n    env_name=env_name,                 # aus deinem Setup\n    video_dir=\"videos\",                # Ordner fÃ¼r Video\n    max_steps=max_steps_per_episode,   # aus deinem Setup\n    device=\"cpu\"                       # falls GPU nicht verfÃ¼gbar\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:27:17.318821Z","iopub.execute_input":"2026-01-10T12:27:17.319698Z","iopub.status.idle":"2026-01-10T12:27:19.198393Z","shell.execute_reply.started":"2026-01-10T12:27:17.319667Z","shell.execute_reply":"2026-01-10T12:27:19.197641Z"}},"outputs":[{"name":"stdout","text":"Lade Modell aus 'training_history/LunarLander-v3_2026-01-10_12-12-52/checkpoint_generation_16' mit Layer-Info: [96, 128, 40, 18]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n","output_type":"stream"},{"name":"stdout","text":"Episode LÃ¤nge: 227\nReturn: 295.5873778021281\nVideo gespeichert in: videos\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# !rm -rf checkpoints/\n# !rm -rf training_history/\n# !rm -rf videos/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:26:22.960434Z","iopub.status.idle":"2026-01-10T12:26:22.960733Z","shell.execute_reply.started":"2026-01-10T12:26:22.960584Z","shell.execute_reply":"2026-01-10T12:26:22.960599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}