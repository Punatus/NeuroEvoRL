{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gymnasium\n# !pip install \"gymnasium[atari, accept-rom-license]\"\n# !apt-get install -y swig\n# !pip install gymnasium[box2d]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.049433Z","iopub.execute_input":"2026-01-10T11:47:43.049854Z","iopub.status.idle":"2026-01-10T11:47:43.054440Z","shell.execute_reply.started":"2026-01-10T11:47:43.049827Z","shell.execute_reply":"2026-01-10T11:47:43.053406Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ===========================\n# IMPORTS & GLOBAL SETUP\n# ===========================\nimport os\nimport time\nimport json\nimport csv\nimport random\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nimport multiprocessing as mp\nfrom collections import deque\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nfrom collections import deque, namedtuple\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.056828Z","iopub.execute_input":"2026-01-10T11:47:43.057301Z","iopub.status.idle":"2026-01-10T11:47:43.076489Z","shell.execute_reply.started":"2026-01-10T11:47:43.057273Z","shell.execute_reply":"2026-01-10T11:47:43.075106Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import warnings\nimport logging\n\n# -----------------------\n# WARNINGS UNTERDRÃœCKEN\n# -----------------------\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Gym / MoviePy Logging leiser machen\nlogging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\nlogging.getLogger(\"moviepy\").setLevel(logging.ERROR)\n\n# SDL / Pygame Headless Fix (verhindert XDG_RUNTIME_DIR Meldung)\nos.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.077615Z","iopub.execute_input":"2026-01-10T11:47:43.078227Z","iopub.status.idle":"2026-01-10T11:47:43.098079Z","shell.execute_reply.started":"2026-01-10T11:47:43.078149Z","shell.execute_reply":"2026-01-10T11:47:43.097036Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"env_name = \"LunarLander-v2\"\nenv = gym.make(env_name)\nstate_shape = env.observation_space.shape\nstate_size = env.observation_space.shape[0]\nnumber_actions = env.action_space.n\nprint('State shape: ', state_shape)\nprint('State size: ', state_size)\nprint('Number of actions: ', number_actions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.099440Z","iopub.execute_input":"2026-01-10T11:47:43.100315Z","iopub.status.idle":"2026-01-10T11:47:43.775568Z","shell.execute_reply.started":"2026-01-10T11:47:43.100283Z","shell.execute_reply":"2026-01-10T11:47:43.774093Z"}},"outputs":[{"name":"stdout","text":"State shape:  (8,)\nState size:  8\nNumber of actions:  4\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    \"\"\"\n    Dynamisches MLP fÃ¼r DQN.\n    Linear + ReLU nur zwischen Hidden-Layern\n    \"\"\"\n    def __init__(self, state_dim, action_dim, layer_sizes=[64, 8, 64, 64]):\n        super().__init__()\n        torch.manual_seed(42)\n\n        layers = []\n        input_size = state_dim\n        for i, size in enumerate(layer_sizes):\n            layers.append(nn.Linear(input_size, size))\n            # ReLU nur nach Hidden-Layern, nicht nach dem letzten Layer\n            if i != len(layer_sizes) - 1:\n                layers.append(nn.ReLU())\n            input_size = size\n\n        # Output Layer\n        layers.append(nn.Linear(input_size, action_dim))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.777968Z","iopub.execute_input":"2026-01-10T11:47:43.778326Z","iopub.status.idle":"2026-01-10T11:47:43.785637Z","shell.execute_reply.started":"2026-01-10T11:47:43.778302Z","shell.execute_reply":"2026-01-10T11:47:43.784755Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class ReplayBuffer:\n    \"\"\"\n    Speichert vergangene Erfahrungen:\n    (state, action, reward, next_state, done)\n\n    Warum?\n    - bricht zeitliche Korrelationen\n    - alte Erfahrungen kÃ¶nnen mehrfach gelernt werden\n    \"\"\"\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, experience):\n        \"\"\"\n        FÃ¼gt eine Erfahrung hinzu.\n        Wenn der Speicher voll ist â†’ die Ã¤lteste fliegt raus.\n        \"\"\"\n        self.memory.append(experience)\n        if len(self.memory) > self.capacity:\n            self.memory.pop(0)\n\n    def sample(self, batch_size):\n        \"\"\"\n        Zieht zufÃ¤llige Erfahrungen fÃ¼r ein Lern-Update\n        \"\"\"\n        batch = random.sample(self.memory, batch_size)\n\n        states      = torch.tensor(np.vstack([e[0] for e in batch]), dtype=torch.float32).to(self.device)\n        actions     = torch.tensor(np.vstack([e[1] for e in batch]), dtype=torch.long).to(self.device)\n        rewards     = torch.tensor(np.vstack([e[2] for e in batch]), dtype=torch.float32).to(self.device)\n        next_states = torch.tensor(np.vstack([e[3] for e in batch]), dtype=torch.float32).to(self.device)\n        dones       = torch.tensor(np.vstack([e[4] for e in batch]), dtype=torch.float32).to(self.device)\n\n        return states, actions, rewards, next_states, dones\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.786573Z","iopub.execute_input":"2026-01-10T11:47:43.786908Z","iopub.status.idle":"2026-01-10T11:47:43.807203Z","shell.execute_reply.started":"2026-01-10T11:47:43.786881Z","shell.execute_reply":"2026-01-10T11:47:43.806123Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class DQNAgent:\n    \"\"\"\n    DQN-Agent mit dynamischem QNetwork (beliebige Layer-Architektur)\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        learning_rate,\n        buffer_size,\n        batch_size,\n        gamma,\n        tau,\n        layer_info=None,  # <-- Neue Option fÃ¼r dynamische Architekturen\n    ):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.batch_size = batch_size\n        self.gamma = gamma          # Discount-Faktor\n        self.tau = tau              # Soft-Update-Faktor bzw interpolation faktor\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.seed = SEED\n\n        # Standard-Layer falls nichts angegeben\n        if layer_info is None:\n            layer_info = [64, 64, 64]\n\n        # Online-Netzwerk\n        self.q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n\n        # Target-Netzwerk\n        self.target_q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n        self.target_q_network.load_state_dict(self.q_network.state_dict())  # initial synchronisieren\n\n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n        # Replay Buffer\n        self.replay_buffer = ReplayBuffer(buffer_size)\n\n        self.step_counter = 0\n\n    def select_action(self, state, epsilon = 0.):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        self.q_network.eval()\n        with torch.no_grad():\n            q_values = self.q_network(state)\n        self.q_network.train()\n\n        if random.random() > epsilon:\n            return q_values.argmax(dim=1).item()\n        else:\n            return random.randrange(self.action_dim)\n\n    def step(self, state, action, reward, next_state, done):\n        self.replay_buffer.add((state, action, reward, next_state, done))\n        self.step_counter += 1\n\n        # Nicht bei jedem Schritt lernen (stabiler)\n        if self.step_counter % 4 == 0:\n            if len(self.replay_buffer.memory) >= self.batch_size:\n                batch = self.replay_buffer.sample(self.batch_size)\n                self.learn(batch)\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states, dones = experiences\n\n        with torch.no_grad():\n            next_q_values = self.target_q_network(next_states).max(dim=1, keepdim=True)[0]\n            q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n\n        q_expected = self.q_network(states).gather(1, actions)\n\n        loss = F.mse_loss(q_expected, q_targets)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.soft_update()\n\n    def soft_update(self):\n        for target_param, local_param in zip(\n            self.target_q_network.parameters(),\n            self.q_network.parameters()\n        ):\n            target_param.data.copy_(\n                self.tau * local_param.data +\n                (1.0 - self.tau) * target_param.data\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.808420Z","iopub.execute_input":"2026-01-10T11:47:43.808852Z","iopub.status.idle":"2026-01-10T11:47:43.838770Z","shell.execute_reply.started":"2026-01-10T11:47:43.808821Z","shell.execute_reply":"2026-01-10T11:47:43.837615Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_layer_info(agent_or_model):\n    \"\"\"\n    Gibt nur die GrÃ¶ÃŸe der Linear-Layer zurÃ¼ck, z.B. [64, 64]\n    \"\"\"\n    if hasattr(agent_or_model, \"q_network\"):\n        model = agent_or_model.q_network\n    else:\n        model = agent_or_model\n\n    layer_sizes = []\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear) and layer.out_features != model.model[-1].out_features:\n            # Letzter Linear-Layer (Output) ignorieren\n            layer_sizes.append(layer.out_features)\n    return layer_sizes\n\n# -------------------------------\n# load_model_bundle: rekonstruiert Linear+ReLU Modell\n# -------------------------------\ndef save_model_bundle(model, save_dir, state_dim, action_dim, fitness=None, generation=None):\n    os.makedirs(save_dir, exist_ok=True)\n    # 1ï¸âƒ£ Weights speichern\n    torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n\n    # 2ï¸âƒ£ Meta speichern\n    meta = {\n        \"state_dim\": int(state_dim),\n        \"action_dim\": int(action_dim),\n        \"layer_info\": get_layer_info(model),\n        \"fitness\": float(fitness) if fitness is not None else None,\n        \"generation\": int(generation) if generation is not None else None,\n        \"seed\": int(SEED) if SEED is not None else None,\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    with open(os.path.join(save_dir, \"meta.json\"), \"w\") as f:\n        json.dump(meta, f, indent=4)\n    print(f\"âœ“ Model saved to {save_dir}\")\n\ndef load_model_bundle(model_dir, device=\"cpu\"):\n    # Meta laden\n    with open(os.path.join(model_dir, \"meta.json\"), \"r\") as f:\n        meta = json.load(f)\n    state_dim = meta[\"state_dim\"]\n    action_dim = meta[\"action_dim\"]\n    layer_info = meta[\"layer_info\"]\n\n    # Modell rekonstruieren\n    model = QNetwork(state_dim, action_dim, layer_info).to(device)\n\n    # Gewichte laden (reihenfolgebasiert)\n    state_dict_saved = torch.load(os.path.join(model_dir, \"model.pth\"), map_location=device)\n    state_dict_model = model.state_dict()\n    for k_model, k_saved in zip(state_dict_model.keys(), state_dict_saved.keys()):\n        state_dict_model[k_model] = state_dict_saved[k_saved]\n    model.load_state_dict(state_dict_model)\n\n    return model, meta\n\n\ndef init_training_run(env_name, base_dir=\"training_history\"):\n    \"\"\"\n    Erstellt einen neuen Run-Ordner mit Timestamp\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    run_dir = os.path.join(base_dir, f\"{env_name}_{timestamp}\")\n    os.makedirs(run_dir, exist_ok=True)\n\n    print(f\"ğŸ“ Neuer Trainingslauf: {run_dir}\")\n    return run_dir\n\ndef init_training_log(run_dir):\n    \"\"\"\n    Erstellt CSV-Logdatei fÃ¼r einen Trainingslauf\n    \"\"\"\n    csv_path = os.path.join(run_dir, \"training_log.csv\")\n\n    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n        writer.writerow([\n            \"Environment\",\n            \"Generation\",\n            \"Model_Architecture\",\n            \"Fitness\",\n            \"Trained_Episodes\",\n        ])\n\n    print(f\"ğŸ“Š Training-Log erstellt: {csv_path}\")\n    return csv_path\n    \ndef append_generation_log(\n    csv_path,\n    env_name,\n    generation,\n    hidden_sizes,\n    fitness,\n    episodes_per_individual,\n):\n    with open(csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n\n        writer.writerow([\n            env_name,\n            generation,\n            str(hidden_sizes),\n            f\"{fitness:.2f}\",\n            generation * episodes_per_individual,\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.840749Z","iopub.execute_input":"2026-01-10T11:47:43.841099Z","iopub.status.idle":"2026-01-10T11:47:43.868915Z","shell.execute_reply.started":"2026-01-10T11:47:43.841069Z","shell.execute_reply":"2026-01-10T11:47:43.867363Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# =========================\n# ğŸš€ MUTATION\n# =========================\nclass PopulationMember:\n    def __init__(self, agent):\n        self.agent = agent\n        self.total_reward = 0.0\n        self.episode_rewards = []\n\ndef create_initial_population(\n    population_size,\n    state_dim,\n    action_dim,\n    min_neurons=8,\n    max_neurons=128,\n    learning_rate=5e-4,\n    buffer_size=int(1e5),\n    batch_size=100,\n    gamma=0.99,\n    tau=1e-3,\n):\n    population = []\n    for _ in range(population_size):\n        num_layers = random.randint(min_layers, max_layers)\n        layers = [random.randint(min_neurons, max_neurons) for _ in range(num_layers)]\n        agent = DQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            learning_rate=learning_rate,\n            buffer_size=buffer_size,\n            batch_size=batch_size,\n            gamma=gamma,\n            tau=tau,\n            layer_info=layers\n        )\n        population.append(PopulationMember(agent))\n    return population\n\n\ndef evaluate_population(population):\n    return sorted(\n        population,\n        key=lambda p: p.total_reward,\n        reverse=True,\n    )\n\ndef select_parents(sorted_population):\n    return sorted_population[0], sorted_population[1]\n\ndef compute_mutation_count(epsilon, mutation_factor, max_mutations):\n    return max(\n        1,\n        min(\n            max_mutations,\n            int(mutation_factor * epsilon),\n        ),\n    )\n\ndef partial_load_state_dict(target_model, source_state_dict):\n    \"\"\"\n    LÃ¤dt nur kompatible Gewichte (gleicher Name + gleiche Shape).\n    \"\"\"\n    target_state = target_model.state_dict()\n    compatible_weights = {}\n\n    for name, param in source_state_dict.items():\n        if name in target_state and target_state[name].shape == param.shape:\n            compatible_weights[name] = param\n\n    target_model.load_state_dict(compatible_weights, strict=False)\n\n\ndef create_child_from_parent(parent_agent, n_mutations):\n    # ğŸ§¬ Architektur mutieren\n    new_layers = mutate_architecture_n_times(\n        get_layer_info(parent_agent),\n        n=n_mutations,\n    )\n\n    # ğŸ—ï¸ Neues Modell bauen\n    child_model = build_model_from_architecture(\n        model_class=QNetwork,\n        state_dim=parent_agent.state_dim,\n        action_dim=parent_agent.action_dim,\n        hidden_layers=new_layers,\n        device=parent_agent.device,\n    )\n\n    # â™»ï¸ Gewichte Ã¼bernehmen wo mÃ¶glich\n    partial_load_state_dict(\n        child_model,\n        parent_agent.qnetwork_local.state_dict()\n    )\n\n    # ğŸ‘¶ Neues Agenten-Objekt\n    child_agent = DQNAgent(\n        state_dim=parent_agent.state_dim,\n        action_dim=parent_agent.action_dim,\n        learning_rate=parent_agent.learning_rate,\n        buffer_size=parent_agent.buffer_size,\n        batch_size=parent_agent.batch_size,\n        gamma=parent_agent.gamma,\n        tau=parent_agent.tau,\n        layer_info=new_layers,\n        device=parent_agent.device,\n    )\n\n    # ğŸ” Mutiertes Netz einsetzen\n    child_agent.qnetwork_local = child_model\n\n    # ğŸ¯ Target-Netz initial angleichen (WICHTIG!)\n    child_agent.qnetwork_target.load_state_dict(\n        child_model.state_dict()\n    )\n\n    # Îµ vom Elternteil Ã¼bernehmen\n    child_agent.epsilon = parent_agent.epsilon\n\n    return PopulationMember(child_agent)\n\ndef check_architecture_dominance(\n    architecture_win_counter,\n    threshold=6,\n):\n    for arch, wins in architecture_win_counter.items():\n        if wins >= threshold:\n            return list(arch)\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.870107Z","iopub.execute_input":"2026-01-10T11:47:43.870921Z","iopub.status.idle":"2026-01-10T11:47:43.899409Z","shell.execute_reply.started":"2026-01-10T11:47:43.870890Z","shell.execute_reply":"2026-01-10T11:47:43.898030Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def mutate_architecture(\n    layer_info, \n    n_mutations=1, \n    min_neurons=8, \n    max_neurons=128\n):\n    \"\"\"\n    Mutiert ein Layer-Array n_mutations Mal.\n    Optionen pro Mutation:\n    1ï¸âƒ£ Dupliziere oder lÃ¶sche eine zufÃ¤llige Schicht (Layer-Anzahl begrenzt auf min_layers/max_layers)\n    2ï¸âƒ£ Neuronen *2 oder /2 (begrenzt auf min_neurons/max_neurons)\n    \"\"\"\n    new_layers = layer_info.copy()\n    for _ in range(n_mutations):\n        if len(new_layers) == 0:\n            # Falls alle Layer gelÃ¶scht wurden, mindestens 1 Layer wieder hinzufÃ¼gen\n            new_layers.append(min_neurons)\n            continue\n\n        idx = random.randint(0, len(new_layers)-1)\n        op = random.choice([\"layer_op\", \"scale\"])\n\n        if op == \"layer_op\":\n            # Duplizieren oder lÃ¶schen, Layer-Anzahl beachten\n            if len(new_layers) == 1:\n                # Nur duplizieren mÃ¶glich\n                if len(new_layers) < max_layers:\n                    new_layers.insert(idx, new_layers[idx])\n            else:\n                if random.random() < 0.5:\n                    # duplizieren (nur wenn max_layers nicht Ã¼berschritten)\n                    if len(new_layers) < max_layers:\n                        new_layers.insert(idx, new_layers[idx])\n                else:\n                    # lÃ¶schen (nur wenn min_layers nicht unterschritten)\n                    if len(new_layers) > min_layers:\n                        new_layers.pop(idx)\n        else:\n            # Neuronen *2 oder /2\n            factor = random.choice([0.5, 2])\n            new_layers[idx] = max(min_neurons, min(max_neurons, int(new_layers[idx] * factor)))\n\n    return new_layers\n\n\ndef create_initial_population(population_size, state_dim, action_dim, min_layers=1, max_layers=10,\n                              min_neurons=8, max_neurons=128):\n    \"\"\"\n    Initialisiert Population. Layer-GrÃ¶ÃŸe als Faktor von min_neurons.\n    \"\"\"\n    population = []\n    for _ in range(population_size):\n        num_layers = random.randint(min_layers, max_layers)\n        layers = [random.randint(1, max_neurons // min_neurons) * min_neurons for _ in range(num_layers)]\n        agent = DQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            learning_rate=learning_rate,\n            buffer_size=buffer_size,\n            batch_size=batch_size,\n            gamma=gamma,\n            tau=tau,\n            layer_info=layers\n        )\n        population.append(PopulationMember(agent))\n    return population\n\ndef create_child_from_parents(parent_agent, mutation_factor=5, epsilon=1, min_mutations=1):\n    n_mutations = max(min_mutations, int(mutation_factor * epsilon))\n    new_layers = mutate_architecture(get_layer_info(parent_agent), n_mutations=n_mutations)\n    child_agent = DQNAgent(\n        state_dim=state_size,\n        action_dim=number_actions,\n        learning_rate=learning_rate,\n        buffer_size=buffer_size,\n        batch_size=batch_size,\n        gamma=gamma,\n        tau=tau,\n        layer_info=new_layers\n    )\n    return PopulationMember(child_agent)\n\ndef evaluate_population(env, population, max_steps=1000):\n    \"\"\"\n    FÃ¼hrt jede Population durch die Environment aus und speichert Reward.\n    \"\"\"\n    for member in population:\n        reset_out = env.reset()\n        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n        total_reward = 0\n        for _ in range(max_steps):\n            action = member.agent.select_action(state, epsilon)\n            step_out = env.step(action)\n            if len(step_out) == 5:\n                state, reward, terminated, truncated, _ = step_out\n                done = terminated or truncated\n            else:\n                state, reward, done, _ = step_out\n            total_reward += reward\n            if done:\n                break\n        member.total_reward = total_reward\n    return sorted(population, key=lambda m: m.total_reward, reverse=True)\n\ndef check_architecture_dominance(win_counter, threshold=6):\n    for arch, count in win_counter.items():\n        if count >= threshold:\n            return list(arch)\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.900740Z","iopub.execute_input":"2026-01-10T11:47:43.901106Z","iopub.status.idle":"2026-01-10T11:47:43.930835Z","shell.execute_reply.started":"2026-01-10T11:47:43.901071Z","shell.execute_reply":"2026-01-10T11:47:43.929515Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_agent_for_episodes(\n    env,\n    agent,\n    num_episodes,\n    generation,\n    max_steps_per_episode,\n    epsilon_start,\n    epsilon_end,\n    epsilon_decay,\n):\n    \"\"\"\n    Trainiert einen Agenten fÃ¼r num_episodes Episoden.\n    Îµ ist strikt lokal.\n    Gibt Mean-Reward zurÃ¼ck.\n    \"\"\"\n    epsilon = epsilon_start\n    rewards = []\n\n    for ep in range(num_episodes):\n        reset_out = env.reset(seed=generation * 10_000 + ep)\n        state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n        episode_reward = 0.0\n\n        for _ in range(max_steps_per_episode):\n            action = agent.select_action(state, epsilon)\n\n            step_out = env.step(action)\n            if len(step_out) == 5:\n                next_state, reward, terminated, truncated, _ = step_out\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = step_out\n\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                break\n\n        rewards.append(episode_reward)\n\n        # Îµ nur lokal decayn\n        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n\n    return float(np.mean(rewards))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.932253Z","iopub.execute_input":"2026-01-10T11:47:43.933303Z","iopub.status.idle":"2026-01-10T11:47:43.959152Z","shell.execute_reply.started":"2026-01-10T11:47:43.933261Z","shell.execute_reply":"2026-01-10T11:47:43.957580Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def run_trained_model_and_record(\n    model_dir,\n    env_name=\"CartPole-v1\",\n    video_dir=\"videos\",\n    max_steps=500,\n    device=\"cpu\"\n):\n\n    os.makedirs(video_dir, exist_ok=True)\n\n    # --------\n    # Modell + Metadaten laden\n    # --------\n    model, meta = load_model_bundle(model_dir, device=device)\n    model.eval()\n    print(f\"Lade Modell aus '{model_dir}' mit Layer-Info: {meta['layer_info']}\")\n\n    # --------\n    # ENV vorbereiten + Video-Wrapper\n    # --------\n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    env = RecordVideo(\n        env,\n        video_folder=video_dir,\n        episode_trigger=lambda ep: True,  # jedes Episode aufnehmen\n        name_prefix=\"episode\"\n    )\n\n    reset_out = env.reset()\n    state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n    total_reward = 0\n\n    # --------\n    # Episode ausfÃ¼hren\n    # --------\n    for step in range(max_steps):\n        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            q_values = model(state_t)\n            action = torch.argmax(q_values, dim=-1).item()\n\n        step_out = env.step(action)\n        if len(step_out) == 5:\n            state, reward, terminated, truncated, _ = step_out\n            done = terminated or truncated\n        else:\n            state, reward, done, _ = step_out\n\n        total_reward += reward\n        if done:\n            break\n\n    env.close()\n\n    print(f\"Episode LÃ¤nge: {step+1}\")\n    print(f\"Return: {total_reward}\")\n    print(f\"Video gespeichert in: {video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.960479Z","iopub.execute_input":"2026-01-10T11:47:43.960800Z","iopub.status.idle":"2026-01-10T11:47:43.984390Z","shell.execute_reply.started":"2026-01-10T11:47:43.960777Z","shell.execute_reply":"2026-01-10T11:47:43.983251Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"learning_rate = 5e-4\nbuffer_size = int(1e5)\nbatch_size = 50\ngamma = 0.99\ntau = 1e-3\n\n# -----------------------------\n# Ordner + Log initialisieren\n# -----------------------------\nrun_dir = init_training_run(env_name)\nlog_csv = init_training_log(run_dir)\n\n# -----------------------------\n# EvolutionÃ¤re Trainingsparameter\n# -----------------------------\n\npopulation_size = 10\nmutation_factor = 5\nthresholddom = 5\nmin_layers=1 \nmax_layers=5\n\nmax_steps_per_episode = 100\ngoal = 200.0\n\nepsilon_start = 1.0\nepsilon_end = 0.01\nepsilon_decay = 0.995\nepsilon = epsilon_start\nscores_window = deque(maxlen=100)\n\npopulation = create_initial_population(population_size, state_dim=state_size, action_dim=number_actions)\narchitecture_win_counter = defaultdict(int)\ndominant_architecture = None\ngeneration = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:43.985624Z","iopub.execute_input":"2026-01-10T11:47:43.986356Z","iopub.status.idle":"2026-01-10T11:47:47.957434Z","shell.execute_reply.started":"2026-01-10T11:47:43.986305Z","shell.execute_reply":"2026-01-10T11:47:47.956058Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Neuer Trainingslauf: training_history/LunarLander-v2_2026-01-10_11-47-44\nğŸ“Š Training-Log erstellt: training_history/LunarLander-v2_2026-01-10_11-47-44/training_log.csv\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# -----------------------------\n# Trainingsloop\n# -----------------------------\ngeneration = 0\nepsilon = epsilon_start\n\n# =========================\n# ğŸ”¹ Population trainieren\n# =========================\nwhile True:\n    for member in population:\n        mean_score = train_agent_for_episodes(\n            env=env,\n            agent=member.agent,\n            num_episodes=batch_size,\n            generation=generation,\n            max_steps_per_episode=max_steps_per_episode,\n            epsilon_start=epsilon,\n            epsilon_end=epsilon_end,\n            epsilon_decay=epsilon_decay,\n        )\n        member.total_reward = mean_score\n        print(f\"training von {get_layer_info(member.agent)} fertig\")\n        \n    population.sort(key=lambda m: m.total_reward, reverse=True)\n    print(f\"\\nğŸ… Population Ranking â€“ Generation {generation}\")\n    for rank, member in enumerate(population, start=1):\n        print(\n            f\"{rank:2d}. Reward: {member.total_reward:8.2f} | \"\n            f\"Layers: {get_layer_info(member.agent)}\"\n        )\n        append_generation_log(\n            csv_path=log_csv,\n            env_name=env_name,\n            generation=generation,\n            hidden_sizes=get_layer_info(member.agent),\n            fitness=member.total_reward,\n            episodes_per_individual=batch_size\n        )\n    \n    if population[0].total_reward >= goal:\n        # ğŸ”¹ Nur das beste Modell speichern\n        save_dir = os.path.join(run_dir, f\"checkpoint_generation_{generation}\")\n        save_model_bundle(\n            model=population[0].agent.q_network,\n            save_dir=save_dir,\n            state_dim=state_size,\n            action_dim=number_actions,\n            fitness=population[0].total_reward,\n            generation=generation\n        )\n        print(f\"\\nğŸ¯ Goal erreicht mit {get_layer_info(population[0].agent)} in Generation {generation}\")\n        break\n    \n    # Dominanz prÃ¼fen und +\n    architecture_win_counter[tuple(get_layer_info(population[0].agent))] += 1\n    dominant_architecture = check_architecture_dominance(architecture_win_counter, thresholddom)\n    \n    # ğŸ”¹ Neue Population erzeugen\n    new_population = []\n    if dominant_architecture:\n        new_population = [population[0]]  # nur dominant weitertrainieren\n    else:\n        new_population = [population[0], population[1]]\n        remaining_slots = population_size - 2\n        n_children_p1 = int(remaining_slots * 0.5)\n        n_children_p2 = remaining_slots - n_children_p1\n\n        print(f\"Kinder haben {max(1, int(mutation_factor * epsilon))} mutationen\")\n    \n        for _ in range(n_children_p1):\n            child = create_child_from_parents(population[0].agent, mutation_factor, epsilon)\n            new_population.append(child)\n        for _ in range(n_children_p2):\n            child = create_child_from_parents(population[1].agent, mutation_factor, epsilon)\n            new_population.append(child)\n    \n    population = new_population\n    \n    # ğŸ”¹ Epsilon decay\n    epsilon = max(epsilon_end, epsilon * (epsilon_decay ** batch_size))\n    generation += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T11:47:47.960377Z","iopub.execute_input":"2026-01-10T11:47:47.960897Z","iopub.status.idle":"2026-01-10T12:02:56.446998Z","shell.execute_reply.started":"2026-01-10T11:47:47.960869Z","shell.execute_reply":"2026-01-10T12:02:56.445482Z"}},"outputs":[{"name":"stdout","text":"training von [8, 128] fertig\ntraining von [128, 120, 32] fertig\ntraining von [96, 24, 40, 72, 24, 104] fertig\ntraining von [16, 104, 104, 16, 128, 104, 32] fertig\ntraining von [96] fertig\ntraining von [88, 8, 80, 16] fertig\ntraining von [96, 48, 72, 128, 72, 48, 24, 64, 96, 128] fertig\ntraining von [128, 112, 72, 48, 72, 104, 104, 112] fertig\ntraining von [120, 72, 16, 120, 112, 8] fertig\ntraining von [16, 16] fertig\n\nğŸ… Population Ranking â€“ Generation 0\n 1. Reward:  -103.14 | Layers: [8, 128]\n 2. Reward:  -122.18 | Layers: [128, 120, 32]\n 3. Reward:  -128.18 | Layers: [88, 8, 80, 16]\n 4. Reward:  -128.32 | Layers: [120, 72, 16, 120, 112, 8]\n 5. Reward:  -132.68 | Layers: [16, 16]\n 6. Reward:  -140.18 | Layers: [96]\n 7. Reward:  -145.46 | Layers: [16, 104, 104, 16, 128, 104, 32]\n 8. Reward:  -151.17 | Layers: [96, 48, 72, 128, 72, 48, 24, 64, 96, 128]\n 9. Reward:  -153.58 | Layers: [128, 112, 72, 48, 72, 104, 104, 112]\n10. Reward:  -167.78 | Layers: [96, 24, 40, 72, 24, 104]\nKinder haben 5 mutationen\ntraining von [8, 128] fertig\ntraining von [128, 120, 32] fertig\ntraining von [8, 32, 32, 128, 128] fertig\ntraining von [16] fertig\ntraining von [8] fertig\ntraining von [64] fertig\ntraining von [128, 32] fertig\ntraining von [128, 128, 128, 120, 120] fertig\ntraining von [128, 64, 32] fertig\ntraining von [128, 120, 120] fertig\n\nğŸ… Population Ranking â€“ Generation 1\n 1. Reward:   -68.95 | Layers: [128, 120, 32]\n 2. Reward:   -95.29 | Layers: [128, 64, 32]\n 3. Reward:   -97.91 | Layers: [128, 128, 128, 120, 120]\n 4. Reward:  -104.00 | Layers: [8, 128]\n 5. Reward:  -124.97 | Layers: [16]\n 6. Reward:  -143.62 | Layers: [64]\n 7. Reward:  -149.03 | Layers: [128, 32]\n 8. Reward:  -155.48 | Layers: [128, 120, 120]\n 9. Reward:  -183.94 | Layers: [8]\n10. Reward:  -188.20 | Layers: [8, 32, 32, 128, 128]\nKinder haben 3 mutationen\ntraining von [128, 120, 32] fertig\ntraining von [128, 64, 32] fertig\ntraining von [120, 120] fertig\ntraining von [128, 64, 128, 120, 32] fertig\ntraining von [64] fertig\ntraining von [128, 128, 128, 64] fertig\ntraining von [128, 128, 64] fertig\ntraining von [128, 64, 32, 16, 32] fertig\ntraining von [128, 128, 32, 32, 32] fertig\ntraining von [128, 128, 128, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 2\n 1. Reward:   -31.36 | Layers: [128, 120, 32]\n 2. Reward:   -44.49 | Layers: [128, 64, 32]\n 3. Reward:   -78.81 | Layers: [128, 128, 128, 32]\n 4. Reward:   -93.84 | Layers: [128, 128, 128, 64]\n 5. Reward:  -115.53 | Layers: [128, 128, 64]\n 6. Reward:  -121.88 | Layers: [120, 120]\n 7. Reward:  -144.72 | Layers: [64]\n 8. Reward:  -163.97 | Layers: [128, 64, 128, 120, 32]\n 9. Reward:  -166.15 | Layers: [128, 128, 32, 32, 32]\n10. Reward:  -196.36 | Layers: [128, 64, 32, 16, 32]\nKinder haben 3 mutationen\ntraining von [128, 120, 32] fertig\ntraining von [128, 64, 32] fertig\ntraining von [128] fertig\ntraining von [128, 128, 32] fertig\ntraining von [128, 128, 64] fertig\ntraining von [128, 128, 128, 32] fertig\ntraining von [32] fertig\ntraining von [64, 64] fertig\ntraining von [32, 64, 32] fertig\ntraining von [128, 64, 8] fertig\n\nğŸ… Population Ranking â€“ Generation 3\n 1. Reward:    -8.35 | Layers: [128, 120, 32]\n 2. Reward:   -27.20 | Layers: [128, 64, 32]\n 3. Reward:   -71.07 | Layers: [128, 128, 32]\n 4. Reward:   -80.18 | Layers: [128, 128, 64]\n 5. Reward:  -117.59 | Layers: [32]\n 6. Reward:  -195.87 | Layers: [128, 128, 128, 32]\n 7. Reward:  -201.54 | Layers: [128]\n 8. Reward:  -210.37 | Layers: [64, 64]\n 9. Reward:  -256.16 | Layers: [128, 64, 8]\n10. Reward:  -262.83 | Layers: [32, 64, 32]\nKinder haben 2 mutationen\ntraining von [128, 120, 32] fertig\ntraining von [128, 64, 32] fertig\ntraining von [128, 120, 64] fertig\ntraining von [128, 32] fertig\ntraining von [128, 64] fertig\ntraining von [128, 120, 32, 16] fertig\ntraining von [64, 64, 64, 32] fertig\ntraining von [128, 16] fertig\ntraining von [128, 32] fertig\ntraining von [128, 16, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 4\n 1. Reward:     4.21 | Layers: [128, 120, 32]\n 2. Reward:   -11.41 | Layers: [128, 64, 32]\n 3. Reward:   -77.19 | Layers: [128, 16]\n 4. Reward:  -100.16 | Layers: [128, 64]\n 5. Reward:  -113.83 | Layers: [128, 120, 64]\n 6. Reward:  -124.12 | Layers: [128, 32]\n 7. Reward:  -206.84 | Layers: [64, 64, 64, 32]\n 8. Reward:  -213.63 | Layers: [128, 32]\n 9. Reward:  -246.91 | Layers: [128, 16, 32]\n10. Reward:  -269.32 | Layers: [128, 120, 32, 16]\nKinder haben 1 mutationen\ntraining von [128, 120, 32] fertig\ntraining von [128, 64, 32] fertig\ntraining von [128, 120] fertig\ntraining von [128, 128, 32] fertig\ntraining von [120, 32] fertig\ntraining von [128, 120, 32, 32] fertig\ntraining von [128, 64, 64] fertig\ntraining von [128, 64, 16] fertig\ntraining von [128, 64] fertig\ntraining von [128, 64, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 5\n 1. Reward:     3.29 | Layers: [128, 120, 32]\n 2. Reward:   -40.96 | Layers: [128, 64, 32]\n 3. Reward:   -51.54 | Layers: [128, 64]\n 4. Reward:  -120.22 | Layers: [128, 128, 32]\n 5. Reward:  -125.37 | Layers: [128, 120]\n 6. Reward:  -138.38 | Layers: [128, 64, 64]\n 7. Reward:  -191.40 | Layers: [120, 32]\n 8. Reward:  -203.04 | Layers: [128, 64, 32]\n 9. Reward:  -243.73 | Layers: [128, 120, 32, 32]\n10. Reward:  -269.20 | Layers: [128, 64, 16]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 6\n 1. Reward:    12.65 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 7\n 1. Reward:    17.79 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 8\n 1. Reward:    23.15 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 9\n 1. Reward:    21.45 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 10\n 1. Reward:    27.14 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 11\n 1. Reward:    29.72 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 12\n 1. Reward:    27.24 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 13\n 1. Reward:    29.72 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 14\n 1. Reward:    28.08 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 15\n 1. Reward:    29.65 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 16\n 1. Reward:    21.34 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 17\n 1. Reward:    27.12 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 18\n 1. Reward:    28.53 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 19\n 1. Reward:    18.05 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 20\n 1. Reward:    23.90 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 21\n 1. Reward:    25.97 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 22\n 1. Reward:    31.36 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 23\n 1. Reward:    32.09 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 24\n 1. Reward:    38.21 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 25\n 1. Reward:    34.58 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 26\n 1. Reward:    40.77 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 27\n 1. Reward:    42.35 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 28\n 1. Reward:    43.02 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 29\n 1. Reward:    37.19 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 30\n 1. Reward:    39.91 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 31\n 1. Reward:    39.12 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 32\n 1. Reward:    48.10 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 33\n 1. Reward:    52.11 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 34\n 1. Reward:    54.06 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 35\n 1. Reward:    52.50 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 36\n 1. Reward:    57.00 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 37\n 1. Reward:    56.37 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 38\n 1. Reward:    55.58 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 39\n 1. Reward:    57.04 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 40\n 1. Reward:    53.74 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 41\n 1. Reward:    57.68 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 42\n 1. Reward:    57.87 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 43\n 1. Reward:    57.39 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 44\n 1. Reward:    50.66 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 45\n 1. Reward:    56.40 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 46\n 1. Reward:    61.79 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 47\n 1. Reward:    58.08 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 48\n 1. Reward:    57.34 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 49\n 1. Reward:    59.20 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 50\n 1. Reward:    58.80 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 51\n 1. Reward:    65.21 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 52\n 1. Reward:    62.94 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 53\n 1. Reward:    62.40 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 54\n 1. Reward:    65.58 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 55\n 1. Reward:    67.29 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 56\n 1. Reward:    71.96 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 57\n 1. Reward:    65.55 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 58\n 1. Reward:    69.64 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 59\n 1. Reward:    75.78 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 60\n 1. Reward:    75.49 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 61\n 1. Reward:    74.81 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 62\n 1. Reward:    75.21 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 63\n 1. Reward:    72.11 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 64\n 1. Reward:    77.44 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 65\n 1. Reward:    74.51 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 66\n 1. Reward:    71.02 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 67\n 1. Reward:    82.16 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 68\n 1. Reward:    78.22 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 69\n 1. Reward:    75.79 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 70\n 1. Reward:    74.66 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 71\n 1. Reward:    78.57 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 72\n 1. Reward:    72.86 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 73\n 1. Reward:    71.52 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 74\n 1. Reward:    76.04 | Layers: [128, 120, 32]\ntraining von [128, 120, 32] fertig\n\nğŸ… Population Ranking â€“ Generation 75\n 1. Reward:    68.94 | Layers: [128, 120, 32]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1922685599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmember\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         mean_score = train_agent_for_episodes(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2302803729.py\u001b[0m in \u001b[0;36mtrain_agent_for_episodes\u001b[0;34m(env, agent, num_episodes, generation, max_steps_per_episode, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mstep_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;31m# Generate two random numbers between -1/SCALE and 1/SCALE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mdispersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mm_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;31m# Generate two random numbers between -1/SCALE and 1/SCALE.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mdispersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mm_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# Angenommen, das Modell wurde gespeichert in:\nmodel_dir = \"training_history/LunarLander-v3_2026-01-09_17-44-30/checkpoint_generation_9\"\n\n# Video ausfÃ¼hren und speichern\nrun_trained_model_and_record(\n    model_dir=model_dir,\n    env_name=env_name,                 # aus deinem Setup\n    video_dir=\"videos\",                # Ordner fÃ¼r Video\n    max_steps=max_steps_per_episode,   # aus deinem Setup\n    device=\"cpu\"                       # falls GPU nicht verfÃ¼gbar\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:02:56.447928Z","iopub.status.idle":"2026-01-10T12:02:56.448416Z","shell.execute_reply.started":"2026-01-10T12:02:56.448191Z","shell.execute_reply":"2026-01-10T12:02:56.448209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf checkpoints/\n# !rm -rf training_history/\n# !rm -rf videos/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:02:56.450265Z","iopub.status.idle":"2026-01-10T12:02:56.450774Z","shell.execute_reply.started":"2026-01-10T12:02:56.450545Z","shell.execute_reply":"2026-01-10T12:02:56.450566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}