{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install gymnasium\n# !pip install \"gymnasium[atari, accept-rom-license]\"\n# !apt-get install -y swig\n# !pip install gymnasium[box2d]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:03:59.349623Z","iopub.execute_input":"2026-01-09T13:03:59.350015Z","iopub.status.idle":"2026-01-09T13:03:59.355076Z","shell.execute_reply.started":"2026-01-09T13:03:59.349983Z","shell.execute_reply":"2026-01-09T13:03:59.354026Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ===========================\n# IMPORTS & GLOBAL SETUP\n# ===========================\nimport os\nimport time\nimport json\nimport csv\nimport random\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nimport multiprocessing as mp\nfrom collections import deque\nfrom datetime import datetime\n\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\nfrom collections import deque, namedtuple\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:41.801365Z","iopub.execute_input":"2026-01-09T13:39:41.801705Z","iopub.status.idle":"2026-01-09T13:39:41.809393Z","shell.execute_reply.started":"2026-01-09T13:39:41.801682Z","shell.execute_reply":"2026-01-09T13:39:41.807503Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"env = gym.make('LunarLander-v2')\nstate_shape = env.observation_space.shape\nstate_size = env.observation_space.shape[0]\nnumber_actions = env.action_space.n\nprint('State shape: ', state_shape)\nprint('State size: ', state_size)\nprint('Number of actions: ', number_actions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:41.980080Z","iopub.execute_input":"2026-01-09T13:39:41.980413Z","iopub.status.idle":"2026-01-09T13:39:41.989218Z","shell.execute_reply.started":"2026-01-09T13:39:41.980393Z","shell.execute_reply":"2026-01-09T13:39:41.987357Z"}},"outputs":[{"name":"stdout","text":"State shape:  (8,)\nState size:  8\nNumber of actions:  4\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import warnings\nimport logging\n\n# -----------------------\n# WARNINGS UNTERDR√úCKEN\n# -----------------------\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Gym / MoviePy Logging leiser machen\nlogging.getLogger(\"gymnasium\").setLevel(logging.ERROR)\nlogging.getLogger(\"moviepy\").setLevel(logging.ERROR)\n\n# SDL / Pygame Headless Fix (verhindert XDG_RUNTIME_DIR Meldung)\nos.environ[\"SDL_AUDIODRIVER\"] = \"dummy\"\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:42.235181Z","iopub.execute_input":"2026-01-09T13:39:42.235464Z","iopub.status.idle":"2026-01-09T13:39:42.242705Z","shell.execute_reply.started":"2026-01-09T13:39:42.235446Z","shell.execute_reply":"2026-01-09T13:39:42.241181Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    \"\"\"\n    Dynamisches MLP f√ºr DQN.\n    Linear + ReLU nur zwischen Hidden-Layern\n    \"\"\"\n    def __init__(self, state_dim, action_dim, layer_sizes=[64, 8, 64, 64]):\n        super().__init__()\n        torch.manual_seed(42)\n\n        layers = []\n        input_size = state_dim\n        for i, size in enumerate(layer_sizes):\n            layers.append(nn.Linear(input_size, size))\n            # ReLU nur nach Hidden-Layern, nicht nach dem letzten Layer\n            if i != len(layer_sizes) - 1:\n                layers.append(nn.ReLU())\n            input_size = size\n\n        # Output Layer\n        layers.append(nn.Linear(input_size, action_dim))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:43.646613Z","iopub.execute_input":"2026-01-09T13:39:43.646911Z","iopub.status.idle":"2026-01-09T13:39:43.654197Z","shell.execute_reply.started":"2026-01-09T13:39:43.646868Z","shell.execute_reply":"2026-01-09T13:39:43.652799Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class ReplayBuffer:\n    \"\"\"\n    Speichert vergangene Erfahrungen:\n    (state, action, reward, next_state, done)\n\n    Warum?\n    - bricht zeitliche Korrelationen\n    - alte Erfahrungen k√∂nnen mehrfach gelernt werden\n    \"\"\"\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def add(self, experience):\n        \"\"\"\n        F√ºgt eine Erfahrung hinzu.\n        Wenn der Speicher voll ist ‚Üí die √§lteste fliegt raus.\n        \"\"\"\n        self.memory.append(experience)\n        if len(self.memory) > self.capacity:\n            self.memory.pop(0)\n\n    def sample(self, batch_size):\n        \"\"\"\n        Zieht zuf√§llige Erfahrungen f√ºr ein Lern-Update\n        \"\"\"\n        batch = random.sample(self.memory, batch_size)\n\n        states      = torch.tensor(np.vstack([e[0] for e in batch]), dtype=torch.float32).to(self.device)\n        actions     = torch.tensor(np.vstack([e[1] for e in batch]), dtype=torch.long).to(self.device)\n        rewards     = torch.tensor(np.vstack([e[2] for e in batch]), dtype=torch.float32).to(self.device)\n        next_states = torch.tensor(np.vstack([e[3] for e in batch]), dtype=torch.float32).to(self.device)\n        dones       = torch.tensor(np.vstack([e[4] for e in batch]), dtype=torch.float32).to(self.device)\n\n        return states, actions, rewards, next_states, dones\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:44.790908Z","iopub.execute_input":"2026-01-09T13:39:44.791261Z","iopub.status.idle":"2026-01-09T13:39:44.801192Z","shell.execute_reply.started":"2026-01-09T13:39:44.791241Z","shell.execute_reply":"2026-01-09T13:39:44.799805Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class DQNAgent:\n    \"\"\"\n    DQN-Agent mit dynamischem QNetwork (beliebige Layer-Architektur)\n    \"\"\"\n\n    def __init__(\n        self,\n        state_dim,\n        action_dim,\n        learning_rate,\n        buffer_size,\n        batch_size,\n        gamma,\n        tau,\n        layer_info=None,  # <-- Neue Option f√ºr dynamische Architekturen\n    ):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.batch_size = batch_size\n        self.gamma = gamma          # Discount-Faktor\n        self.tau = tau              # Soft-Update-Faktor bzw interpolation faktor\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.seed = SEED\n\n        # Standard-Layer falls nichts angegeben\n        if layer_info is None:\n            layer_info = [64, 64, 64]\n\n        # Online-Netzwerk\n        self.q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n\n        # Target-Netzwerk\n        self.target_q_network = QNetwork(state_dim, action_dim, layer_info).to(self.device)\n        self.target_q_network.load_state_dict(self.q_network.state_dict())  # initial synchronisieren\n\n        # Optimizer\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n\n        # Replay Buffer\n        self.replay_buffer = ReplayBuffer(buffer_size)\n\n        self.step_counter = 0\n\n    def select_action(self, state, epsilon = 0.):\n        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n\n        self.q_network.eval()\n        with torch.no_grad():\n            q_values = self.q_network(state)\n        self.q_network.train()\n\n        if random.random() > epsilon:\n            return q_values.argmax(dim=1).item()\n        else:\n            return random.randrange(self.action_dim)\n\n    def step(self, state, action, reward, next_state, done):\n        self.replay_buffer.add((state, action, reward, next_state, done))\n        self.step_counter += 1\n\n        # Nicht bei jedem Schritt lernen (stabiler)\n        if self.step_counter % 4 == 0:\n            if len(self.replay_buffer.memory) >= self.batch_size:\n                batch = self.replay_buffer.sample(self.batch_size)\n                self.learn(batch)\n\n    def learn(self, experiences):\n        states, actions, rewards, next_states, dones = experiences\n\n        with torch.no_grad():\n            next_q_values = self.target_q_network(next_states).max(dim=1, keepdim=True)[0]\n            q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n\n        q_expected = self.q_network(states).gather(1, actions)\n\n        loss = F.mse_loss(q_expected, q_targets)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.soft_update()\n\n    def soft_update(self):\n        for target_param, local_param in zip(\n            self.target_q_network.parameters(),\n            self.q_network.parameters()\n        ):\n            target_param.data.copy_(\n                self.tau * local_param.data +\n                (1.0 - self.tau) * target_param.data\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:55:07.376837Z","iopub.execute_input":"2026-01-09T13:55:07.377127Z","iopub.status.idle":"2026-01-09T13:55:07.390107Z","shell.execute_reply.started":"2026-01-09T13:55:07.377110Z","shell.execute_reply":"2026-01-09T13:55:07.389229Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def get_layer_info(agent_or_model):\n    \"\"\"\n    Gibt nur die Gr√∂√üe der Linear-Layer zur√ºck, z.B. [64, 64]\n    \"\"\"\n    if hasattr(agent_or_model, \"q_network\"):\n        model = agent_or_model.q_network\n    else:\n        model = agent_or_model\n\n    layer_sizes = []\n    for layer in model.modules():\n        if isinstance(layer, nn.Linear) and layer.out_features != model.model[-1].out_features:\n            # Letzter Linear-Layer (Output) ignorieren\n            layer_sizes.append(layer.out_features)\n    return layer_sizes\n\n# -------------------------------\n# load_model_bundle: rekonstruiert Linear+ReLU Modell\n# -------------------------------\ndef save_model_bundle(model, save_dir, state_dim, action_dim, fitness=None, generation=None):\n    os.makedirs(save_dir, exist_ok=True)\n    # 1Ô∏è‚É£ Weights speichern\n    torch.save(model.state_dict(), os.path.join(save_dir, \"model.pth\"))\n\n    # 2Ô∏è‚É£ Meta speichern\n    meta = {\n        \"state_dim\": int(state_dim),\n        \"action_dim\": int(action_dim),\n        \"layer_info\": layer_info,\n        \"fitness\": float(fitness) if fitness is not None else None,\n        \"generation\": int(generation) if generation is not None else None,\n        \"seed\": int(SEED) if SEED is not None else None,\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    with open(os.path.join(save_dir, \"meta.json\"), \"w\") as f:\n        json.dump(meta, f, indent=4)\n    print(f\"‚úì Model saved to {save_dir}\")\n\ndef load_model_bundle(model_dir, device=\"cpu\"):\n    # Meta laden\n    with open(os.path.join(model_dir, \"meta.json\"), \"r\") as f:\n        meta = json.load(f)\n    state_dim = meta[\"state_dim\"]\n    action_dim = meta[\"action_dim\"]\n    layer_info = meta[\"layer_info\"]\n\n    # Modell rekonstruieren\n    model = QNetwork(state_dim, action_dim, layer_info).to(device)\n\n    # Gewichte laden (reihenfolgebasiert)\n    state_dict_saved = torch.load(os.path.join(model_dir, \"model.pth\"), map_location=device)\n    state_dict_model = model.state_dict()\n    for k_model, k_saved in zip(state_dict_model.keys(), state_dict_saved.keys()):\n        state_dict_model[k_model] = state_dict_saved[k_saved]\n    model.load_state_dict(state_dict_model)\n\n    return model, meta\n\n\ndef init_training_run(env_name, base_dir=\"training_history\"):\n    \"\"\"\n    Erstellt einen neuen Run-Ordner mit Timestamp\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    run_dir = os.path.join(base_dir, f\"{env_name}_{timestamp}\")\n    os.makedirs(run_dir, exist_ok=True)\n\n    print(f\"üìÅ Neuer Trainingslauf: {run_dir}\")\n    return run_dir\n\ndef init_training_log(run_dir):\n    \"\"\"\n    Erstellt CSV-Logdatei f√ºr einen Trainingslauf\n    \"\"\"\n    csv_path = os.path.join(run_dir, \"training_log.csv\")\n\n    with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n        writer.writerow([\n            \"Environment\",\n            \"Generation\",\n            \"Model_Architecture\",\n            \"Fitness\",\n            \"Trained_Episodes\",\n            \"Parent_Architecture\"\n        ])\n\n    print(f\"üìä Training-Log erstellt: {csv_path}\")\n    return csv_path\n    \ndef append_generation_log(\n    csv_path,\n    env_name,\n    generation,\n    hidden_sizes,\n    fitness,\n    episodes_per_individual,\n):\n    with open(csv_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.writer(f, delimiter=\";\")\n\n        writer.writerow([\n            env_name,\n            generation,\n            str(hidden_sizes),\n            f\"{fitness:.2f}\",\n            generation * episodes_per_individual,\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:48.382351Z","iopub.execute_input":"2026-01-09T13:39:48.382681Z","iopub.status.idle":"2026-01-09T13:39:48.428968Z","shell.execute_reply.started":"2026-01-09T13:39:48.382660Z","shell.execute_reply":"2026-01-09T13:39:48.427450Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"learning_rate = 5e-4\nbuffer_size = int(1e5)\nbatch_size = 100\ngamma = 0.99\ntau = 1e-3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:51.726014Z","iopub.execute_input":"2026-01-09T13:39:51.726346Z","iopub.status.idle":"2026-01-09T13:39:51.731637Z","shell.execute_reply.started":"2026-01-09T13:39:51.726310Z","shell.execute_reply":"2026-01-09T13:39:51.730818Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"layer_info = [64, 8, 64, 64]\nagent = DQNAgent(\n    state_dim=8,\n    action_dim=4,\n    learning_rate=5e-4,\n    buffer_size=int(1e5),\n    batch_size=100,\n    gamma=0.99,\n    tau=1e-3,\n    layer_info=layer_info\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:51.884401Z","iopub.execute_input":"2026-01-09T13:39:51.885572Z","iopub.status.idle":"2026-01-09T13:39:51.895488Z","shell.execute_reply.started":"2026-01-09T13:39:51.885514Z","shell.execute_reply":"2026-01-09T13:39:51.894554Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"get_layer_info(agent)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:54.307448Z","iopub.execute_input":"2026-01-09T13:39:54.307840Z","iopub.status.idle":"2026-01-09T13:39:54.315373Z","shell.execute_reply.started":"2026-01-09T13:39:54.307808Z","shell.execute_reply":"2026-01-09T13:39:54.314313Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"[64, 8, 64, 64]"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"agent.q_network","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:39:54.861458Z","iopub.execute_input":"2026-01-09T13:39:54.861772Z","iopub.status.idle":"2026-01-09T13:39:54.869260Z","shell.execute_reply.started":"2026-01-09T13:39:54.861750Z","shell.execute_reply":"2026-01-09T13:39:54.868083Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"QNetwork(\n  (model): Sequential(\n    (0): Linear(in_features=8, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=64, out_features=8, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=8, out_features=64, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=64, out_features=64, bias=True)\n    (7): Linear(in_features=64, out_features=4, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"# -----------------------------\n# Trainings-Setup\n# -----------------------------\nnum_episodes = 2000\nmax_steps_per_episode = 1000\n\nepsilon_start = 1.0\nepsilon_end = 0.01\nepsilon_decay = 0.995\nepsilon = epsilon_start\n\nscores_window = deque(maxlen=100)\n\nenv_name = \"LunarLander-v2\"\nenv = gym.make(env_name)\n\ngoal = 200.0\ngoal = -50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:43:16.238311Z","iopub.execute_input":"2026-01-09T13:43:16.238706Z","iopub.status.idle":"2026-01-09T13:43:16.245848Z","shell.execute_reply.started":"2026-01-09T13:43:16.238680Z","shell.execute_reply":"2026-01-09T13:43:16.245111Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# -----------------------------\n# Ordner + Log initialisieren\n# -----------------------------\nrun_dir = init_training_run(env_name)\nlog_csv = init_training_log(run_dir)\n\n# -----------------------------\n# Trainingsloop\n# -----------------------------\nfor episode in range(1, num_episodes + 1):\n    reset_out = env.reset(seed=episode)\n    state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n    episode_reward = 0\n\n    for t in range(max_steps_per_episode):\n        # Aktion ausw√§hlen (Œµ-greedy)\n        action = agent.select_action(state, epsilon)\n\n        # Schritt in der Umwelt\n        step_out = env.step(action)\n        if len(step_out) == 5:\n            next_state, reward, terminated, truncated, _ = step_out\n            done = terminated or truncated\n        else:\n            next_state, reward, done, _ = step_out\n\n        # Erfahrung speichern + ggf. lernen\n        agent.step(state, action, reward, next_state, done)\n        state = next_state\n        episode_reward += reward\n\n        if done:\n            break\n\n    # -----------------------------\n    # Statistik & Epsilon decay\n    # -----------------------------\n    scores_window.append(episode_reward)\n    epsilon = max(epsilon_end, epsilon_decay * epsilon)\n\n    # Live-Ausgabe\n    print(\n        f\"\\rEpisode {episode}\\t\"\n        f\"Average Score: {np.mean(scores_window):.2f}\",\n        end=\"\"\n    )\n\n    if episode % 100 == 0:\n        print(\n            f\"\\rEpisode {episode}\\t\"\n            f\"Average Score: {np.mean(scores_window):.2f}\"\n        )\n\n    # -----------------------------\n    # Modell speichern + Log updaten\n    # -----------------------------\n    if episode % 100 == 0 or np.mean(scores_window) >= goal:\n        save_dir = os.path.join(run_dir, f\"checkpoint_episode_{episode}\")\n        save_model_bundle(\n            model=agent.q_network,\n            save_dir=save_dir,\n            state_dim=env.observation_space.shape[0],\n            action_dim=env.action_space.n,\n            fitness=np.mean(scores_window),\n            generation=episode // 100\n        )\n\n        # Layer-Info korrekt abrufen\n        hidden_layer_info = get_layer_info(agent)  # [(64,'Linear'), (64,'ReLU'), ...]\n        append_generation_log(\n            csv_path=log_csv,\n            env_name=env_name,\n            generation=episode // 100,\n            hidden_sizes=hidden_layer_info,\n            fitness=np.mean(scores_window),\n            episodes_per_individual=episode\n        )\n\n    # Stoppen, wenn Environment gel√∂st\n    if np.mean(scores_window) >= goal:\n        print(\n            f\"\\nüéâ Environment solved in {episode - 100} episodes!\"\n            f\"\\tAverage Score: {np.mean(scores_window):.2f}\"\n        )\n        break\n\nenv.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:43:17.217858Z","iopub.execute_input":"2026-01-09T13:43:17.218178Z","iopub.status.idle":"2026-01-09T13:45:28.920512Z","shell.execute_reply.started":"2026-01-09T13:43:17.218159Z","shell.execute_reply":"2026-01-09T13:45:28.919718Z"}},"outputs":[{"name":"stdout","text":"üìÅ Neuer Trainingslauf: training_history/LunarLander-v2_2026-01-09_13-43-17\nüìä Training-Log erstellt: training_history/LunarLander-v2_2026-01-09_13-43-17/training_log.csv\nEpisode 100\tAverage Score: -195.82\n‚úì Model saved to training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_100\nEpisode 200\tAverage Score: -108.21\n‚úì Model saved to training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_200\nEpisode 300\tAverage Score: -54.455\n‚úì Model saved to training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_300\nEpisode 316\tAverage Score: -48.66‚úì Model saved to training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_316\n\nüéâ Environment solved in 216 episodes!\tAverage Score: -48.66\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"def run_trained_model_and_record(\n    model_dir,\n    env_name=\"CartPole-v1\",\n    video_dir=\"videos\",\n    max_steps=500,\n    device=\"cpu\"\n):\n    import os, gym, torch\n    from gym.wrappers import RecordVideo\n    np.bool8 = np.bool_\n\n    os.makedirs(video_dir, exist_ok=True)\n\n    # --------\n    # Modell + Metadaten laden\n    # --------\n    model, meta = load_model_bundle(model_dir, device=device)\n    model.eval()\n    print(f\"Lade Modell aus '{model_dir}' mit Layer-Info: {meta['layer_info']}\")\n\n    # --------\n    # ENV vorbereiten + Video-Wrapper\n    # --------\n    env = gym.make(env_name, render_mode=\"rgb_array\")\n    env = RecordVideo(\n        env,\n        video_folder=video_dir,\n        episode_trigger=lambda ep: True,  # jedes Episode aufnehmen\n        name_prefix=\"episode\"\n    )\n\n    reset_out = env.reset()\n    state = reset_out[0] if isinstance(reset_out, tuple) else reset_out\n    total_reward = 0\n\n    # --------\n    # Episode ausf√ºhren\n    # --------\n    for step in range(max_steps):\n        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            q_values = model(state_t)\n            action = torch.argmax(q_values, dim=-1).item()\n\n        step_out = env.step(action)\n        if len(step_out) == 5:\n            state, reward, terminated, truncated, _ = step_out\n            done = terminated or truncated\n        else:\n            state, reward, done, _ = step_out\n\n        total_reward += reward\n        if done:\n            break\n\n    env.close()\n\n    print(f\"Episode L√§nge: {step+1}\")\n    print(f\"Return: {total_reward}\")\n    print(f\"Video gespeichert in: {video_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:57:12.224291Z","iopub.execute_input":"2026-01-09T13:57:12.224685Z","iopub.status.idle":"2026-01-09T13:57:12.236749Z","shell.execute_reply.started":"2026-01-09T13:57:12.224655Z","shell.execute_reply":"2026-01-09T13:57:12.235681Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Angenommen, das Modell wurde gespeichert in:\nmodel_dir = \"training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_316\"\n\n# Video ausf√ºhren und speichern\nrun_trained_model_and_record(\n    model_dir=model_dir,\n    env_name=env_name,                 # aus deinem Setup\n    video_dir=\"videos\",                # Ordner f√ºr Video\n    max_steps=max_steps_per_episode,   # aus deinem Setup\n    device=\"cpu\"                       # falls GPU nicht verf√ºgbar\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:57:32.724168Z","iopub.execute_input":"2026-01-09T13:57:32.724562Z","iopub.status.idle":"2026-01-09T13:57:37.413437Z","shell.execute_reply.started":"2026-01-09T13:57:32.724538Z","shell.execute_reply":"2026-01-09T13:57:37.411674Z"}},"outputs":[{"name":"stdout","text":"Lade Modell aus 'training_history/LunarLander-v2_2026-01-09_13-43-17/checkpoint_episode_316' mit Layer-Info: [64, 8, 64, 64]\nEpisode L√§nge: 1000\nReturn: -48.90771214724424\nVideo gespeichert in: videos\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# !rm -rf checkpoints/\n# !rm -rf training_history/\n# !rm -rf videos/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T13:04:03.720378Z","iopub.status.idle":"2026-01-09T13:04:03.720645Z","shell.execute_reply.started":"2026-01-09T13:04:03.720514Z","shell.execute_reply":"2026-01-09T13:04:03.720527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}